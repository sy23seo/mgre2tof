import os
import math
import copy
from pathlib import Path
from random import random
from functools import partial
from collections import namedtuple
from multiprocessing import cpu_count

import torch
from torch import nn, einsum
import torch.nn.functional as F
from torch.nn import Module, ModuleList
from torch.amp import autocast
from torch.utils.data import Dataset, DataLoader

from torch.optim import Adam

from torchvision import transforms as T, utils

from einops import rearrange, reduce, repeat
from einops.layers.torch import Rearrange

from scipy.optimize import linear_sum_assignment

from PIL import Image
from tqdm.auto import tqdm
from ema_pytorch import EMA

from accelerate import Accelerator

from denoising_diffusion_pytorch.attend import Attend

from denoising_diffusion_pytorch.version import __version__

import numpy as np                                                                             
# from denoising_diffusion_pytorch.H5WindowDataset import H5WindowDataset  ## SeoSY 2025-07-24 (dataset classì—ì„œ npyë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•´ì„œì„œ)
# from denoising_diffusion_pytorch.H5WindowDataset_cldm import H5WindowDataset_CLDM  ## SeoSY 2025-10-10 (controlnetìš© dataset class)
from denoising_diffusion_pytorch.H5WindowDataset_unified_1 import H5WindowDataset, H5WindowDataset_CLDM  ## SeoSY 2025-10-13 ìœ„ë‘ê°œë¥´ ã„¹í†µí•©

import contextlib
from tqdm.auto import tqdm as _tqdm

from torch.utils.data import Sampler  # â† ì¶”ê°€

from typing import Optional
from pathlib import Path


from torch.utils.tensorboard import SummaryWriter

# constants
ModelPrediction =  namedtuple('ModelPrediction', ['pred_noise', 'pred_x_start'])

# helpers functions

def exists(x):
    return x is not None

def default(val, d):
    if exists(val):
        return val
    return d() if callable(d) else d

def cast_tuple(t, length = 1):
    if isinstance(t, tuple):
        return t
    return ((t,) * length)

def divisible_by(numer, denom):
    return (numer % denom) == 0

def identity(t, *args, **kwargs):
    return t

def cycle(dl):
    while True:
        for data in dl:
            yield data

def has_int_squareroot(num):
    return (math.sqrt(num) ** 2) == num

def num_to_groups(num, divisor):
    groups = num // divisor
    remainder = num % divisor
    arr = [divisor] * groups
    if remainder > 0:
        arr.append(remainder)
    return arr

def convert_image_to_fn(img_type, image):
    if image.mode != img_type:
        return image.convert(img_type)
    return image

# normalization functions

def normalize_to_neg_one_to_one(img):
    return img * 2 - 1

def unnormalize_to_zero_to_one(t):
    return (t + 1) * 0.5

# small helper modules

def Upsample(dim, dim_out = None):
    return nn.Sequential(
        nn.Upsample(scale_factor = 2, mode = 'nearest'),
        nn.Conv2d(dim, default(dim_out, dim), 3, padding = 1)
    )

def Downsample(dim, dim_out = None):
    return nn.Sequential(
        Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1 = 2, p2 = 2),
        nn.Conv2d(dim * 4, default(dim_out, dim), 1)
    )

class RMSNorm(Module):
    def __init__(self, dim):
        super().__init__()
        self.scale = dim ** 0.5
        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))

    def forward(self, x):
        return F.normalize(x, dim = 1) * self.g * self.scale

# sinusoidal positional embeds

class SinusoidalPosEmb(Module):
    def __init__(self, dim, theta = 10000):
        super().__init__()
        self.dim = dim
        self.theta = theta

    def forward(self, x):
        device = x.device
        half_dim = self.dim // 2
        emb = math.log(self.theta) / (half_dim - 1)
        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)
        emb = x[:, None] * emb[None, :]
        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)
        return emb

class RandomOrLearnedSinusoidalPosEmb(Module):
    """ following @crowsonkb 's lead with random (learned optional) sinusoidal pos emb """
    """ https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 """

    def __init__(self, dim, is_random = False):
        super().__init__()
        assert divisible_by(dim, 2)
        half_dim = dim // 2
        self.weights = nn.Parameter(torch.randn(half_dim), requires_grad = not is_random)

    def forward(self, x):
        x = rearrange(x, 'b -> b 1')
        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi
        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)
        fouriered = torch.cat((x, fouriered), dim = -1)
        return fouriered

# building block modules

class Block(Module):
    def __init__(self, dim, dim_out, dropout = 0.):
        super().__init__()
        self.proj = nn.Conv2d(dim, dim_out, 3, padding = 1)
        self.norm = RMSNorm(dim_out)
        self.act = nn.SiLU()
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, scale_shift = None):
        x = self.proj(x)
        x = self.norm(x)

        if exists(scale_shift):
            scale, shift = scale_shift
            x = x * (scale + 1) + shift

        x = self.act(x)
        return self.dropout(x)

class ResnetBlock(Module):
    def __init__(self, dim, dim_out, *, time_emb_dim = None, dropout = 0.):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.SiLU(),
            nn.Linear(time_emb_dim, dim_out * 2)
        ) if exists(time_emb_dim) else None

        self.block1 = Block(dim, dim_out, dropout = dropout)
        self.block2 = Block(dim_out, dim_out)
        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()

    def forward(self, x, time_emb = None):

        scale_shift = None
        if exists(self.mlp) and exists(time_emb):
            time_emb = self.mlp(time_emb)
            time_emb = rearrange(time_emb, 'b c -> b c 1 1')
            scale_shift = time_emb.chunk(2, dim = 1)

        h = self.block1(x, scale_shift = scale_shift)

        h = self.block2(h)

        return h + self.res_conv(x)

class LinearAttention(Module):
    def __init__(
        self,
        dim,
        heads = 4,
        dim_head = 32,
        num_mem_kv = 4
    ):
        super().__init__()
        self.scale = dim_head ** -0.5
        self.heads = heads
        hidden_dim = dim_head * heads

        self.norm = RMSNorm(dim)

        self.mem_kv = nn.Parameter(torch.randn(2, heads, dim_head, num_mem_kv))
        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)

        self.to_out = nn.Sequential(
            nn.Conv2d(hidden_dim, dim, 1),
            RMSNorm(dim)
        )

    def forward(self, x):
        b, c, h, w = x.shape

        x = self.norm(x)

        qkv = self.to_qkv(x).chunk(3, dim = 1)
        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)', h = self.heads), qkv)

        mk, mv = map(lambda t: repeat(t, 'h c n -> b h c n', b = b), self.mem_kv)
        k, v = map(partial(torch.cat, dim = -1), ((mk, k), (mv, v)))

        q = q.softmax(dim = -2)
        k = k.softmax(dim = -1)

        q = q * self.scale

        context = torch.einsum('b h d n, b h e n -> b h d e', k, v)

        out = torch.einsum('b h d e, b h d n -> b h e n', context, q)
        out = rearrange(out, 'b h c (x y) -> b (h c) x y', h = self.heads, x = h, y = w)
        return self.to_out(out)

class Attention(Module):
    def __init__(
        self,
        dim,
        heads = 4,
        dim_head = 32,
        num_mem_kv = 4,
        flash = False
    ):
        super().__init__()
        self.heads = heads
        hidden_dim = dim_head * heads

        self.norm = RMSNorm(dim)
        self.attend = Attend(flash = flash)

        self.mem_kv = nn.Parameter(torch.randn(2, heads, num_mem_kv, dim_head))
        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)
        self.to_out = nn.Conv2d(hidden_dim, dim, 1)

    def forward(self, x):
        b, c, h, w = x.shape

        x = self.norm(x)

        qkv = self.to_qkv(x).chunk(3, dim = 1)
        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h (x y) c', h = self.heads), qkv)

        mk, mv = map(lambda t: repeat(t, 'h n d -> b h n d', b = b), self.mem_kv)
        k, v = map(partial(torch.cat, dim = -2), ((mk, k), (mv, v)))

        out = self.attend(q, k, v)

        out = rearrange(out, 'b h (x y) d -> b (h d) x y', x = h, y = w)
        return self.to_out(out)

# model
class Unet(Module):
    def __init__(
        self,
        dim,
        init_dim = None,
        out_dim = None,
        dim_mults = (1, 2, 4, 8),
        channels = 3,
        self_condition = False,
        learned_variance = False,
        learned_sinusoidal_cond = False,
        random_fourier_features = False,
        learned_sinusoidal_dim = 16,
        sinusoidal_pos_emb_theta = 10000,
        dropout = 0.,
        attn_dim_head = 32,
        attn_heads = 4,
        full_attn = None,    # defaults to full attention only for inner most layer
        flash_attn = False,
        # -------------------------------------------------20251010 ìˆ˜ì • controlnet update (ì˜µì…˜ ì¶”ê°€)
        use_control: bool = False,
        cond_channels: int | None = None,
        control_inject_down: bool = True,
        control_inject_mid: bool = True,
        control_inject_up: bool = False,          # ê¸°ë³¸ False: ë…¼ë¬¸ ê´€í–‰(ì—… ë¹„ì£¼ì…)
        freeze_locked: bool = True               # ë©”ì¸(locked) ë™ê²° ì—¬ë¶€
    ):
        super().__init__()

        # determine dimensions

        self.channels = channels
        self.self_condition = self_condition
        input_channels = channels * (2 if self_condition else 1)

        init_dim = default(init_dim, dim)
        self.init_conv = nn.Conv2d(input_channels, init_dim, 7, padding = 3) # ì…ë ¥í¬ê¸° ê·¸ëŒ€ë¡œ ìœ ì§€ë¨

        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]
        in_out = list(zip(dims[:-1], dims[1:])) # [(64,128), (128,256), (256,512), (512,1024)]

        # time embeddings

        time_dim = dim * 4

        self.random_or_learned_sinusoidal_cond = learned_sinusoidal_cond or random_fourier_features

        if self.random_or_learned_sinusoidal_cond:
            sinu_pos_emb = RandomOrLearnedSinusoidalPosEmb(learned_sinusoidal_dim, random_fourier_features)
            fourier_dim = learned_sinusoidal_dim + 1
        else:
            sinu_pos_emb = SinusoidalPosEmb(dim, theta = sinusoidal_pos_emb_theta)
            fourier_dim = dim

        self.time_mlp = nn.Sequential(
            sinu_pos_emb,
            nn.Linear(fourier_dim, time_dim),
            nn.GELU(),
            nn.Linear(time_dim, time_dim)
        )

        # attention

        if not full_attn:
            full_attn = (*((False,) * (len(dim_mults) - 1)), True)

        num_stages = len(dim_mults)
        full_attn  = cast_tuple(full_attn, num_stages) # (False, False, False, True)
        attn_heads = cast_tuple(attn_heads, num_stages) # (4, 4, 4, 4)
        attn_dim_head = cast_tuple(attn_dim_head, num_stages)   # (32, 32, 32, 32)

        assert len(full_attn) == len(dim_mults)

        # prepare blocks

        FullAttention = partial(Attention, flash = flash_attn)
        resnet_block = partial(ResnetBlock, time_emb_dim = time_dim, dropout = dropout)

        # layers (LOCKED path = ê¸°ì¡´ ë³¸ì²´)
        self.downs = ModuleList([])
        self.ups = ModuleList([])
        num_resolutions = len(in_out)

        for ind, ((dim_in, dim_out), layer_full_attn, layer_attn_heads, layer_attn_dim_head) in enumerate(zip(in_out, full_attn, attn_heads, attn_dim_head)):
            is_last = ind >= (num_resolutions - 1)

            attn_klass = FullAttention if layer_full_attn else LinearAttention

            self.downs.append(ModuleList([
                resnet_block(dim_in, dim_in),
                resnet_block(dim_in, dim_in),
                attn_klass(dim_in, dim_head = layer_attn_dim_head, heads = layer_attn_heads),
                Downsample(dim_in, dim_out) if not is_last else nn.Conv2d(dim_in, dim_out, 3, padding = 1)
            ]))

        mid_dim = dims[-1]
        self.mid_block1 = resnet_block(mid_dim, mid_dim)
        self.mid_attn = FullAttention(mid_dim, heads = attn_heads[-1], dim_head = attn_dim_head[-1])
        self.mid_block2 = resnet_block(mid_dim, mid_dim)

        for ind, ((dim_in, dim_out), layer_full_attn, layer_attn_heads, layer_attn_dim_head) in enumerate(zip(*map(reversed, (in_out, full_attn, attn_heads, attn_dim_head)))):
            is_last = ind == (len(in_out) - 1)

            attn_klass = FullAttention if layer_full_attn else LinearAttention

            self.ups.append(ModuleList([
                resnet_block(dim_out + dim_in, dim_out),
                resnet_block(dim_out + dim_in, dim_out),
                attn_klass(dim_out, dim_head = layer_attn_dim_head, heads = layer_attn_heads),
                Upsample(dim_out, dim_in) if not is_last else  nn.Conv2d(dim_out, dim_in, 3, padding = 1)
            ]))

        default_out_dim = channels * (1 if not learned_variance else 2)
        self.out_dim = default(out_dim, default_out_dim)

        self.final_res_block = resnet_block(init_dim * 2, init_dim)
        self.final_conv = nn.Conv2d(init_dim, self.out_dim, 1)

        # -------------------------------------------------20251010 ìˆ˜ì • controlnet update (ì»¨íŠ¸ë¡¤ ëª¨ë“œ ì„¤ì •)
        self.use_control = bool(use_control and (cond_channels is not None))
        self.cond_channels = cond_channels
        self.control_inject_down = control_inject_down
        self.control_inject_mid  = control_inject_mid
        self.control_inject_up   = control_inject_up
        self.freeze_locked = freeze_locked

        # -------------------------------------------------20251010 ìˆ˜ì • controlnet update (ControlNet: trainable copy + zero-conv ì•/ë’¤)
        if self.use_control:
            #---------------------------------------2025.10.11 ControlNet ë„ì…ë¶€ ìˆ˜ì •
            # (A) ì–´ë–¤ cond ì±„ë„(K=5/15/64...)ì´ ì™€ë„ ë§ˆì§€ë§‰ ì±„ë„ì„ 64(=dims[0])ë¡œ ì •ë ¬í•˜ëŠ” íŒíŠ¸ ì¸ì½”ë”
            self.control_hint_encoder = nn.Sequential(
                nn.Conv2d(self.cond_channels, 16, 3, padding=1), nn.SiLU(),
                nn.Conv2d(16, 16, 3, padding=1), nn.SiLU(),
                nn.Conv2d(16, 32, 3, padding=1), nn.SiLU(),   # H/2
                nn.Conv2d(32, 96, 3, padding=1), nn.SiLU(),   # H/4
                nn.Conv2d(96, 256, 3, padding=1), nn.SiLU(),  # H/8
                nn.Conv2d(256, dims[0], 3, padding=1)                   # ìµœì¢… 64ch
            )
            # ì´ˆê¸° ì£¼ì… ì˜í–¥ 0ìœ¼ë¡œ ì‹œì‘í•˜ê³  ì‹¶ìœ¼ë©´ ë§ˆì§€ë§‰ convë¥¼ zero-init
            nn.init.zeros_(self.control_hint_encoder[-1].weight)
            nn.init.zeros_(self.control_hint_encoder[-1].bias)
            #---------------------------------------2025.10.11 ControlNet ë„ì…ë¶€ ìˆ˜ì •

            # (A') cond í•´ìƒë„ ì •í•©ìš© ë‹¤ìš´ ê²½ë¡œ (condë¥¼ ê° down-stageì˜ dim_inìœ¼ë¡œ ì´ë™)
            self.control_cond_downsamplers = ModuleList([])
            #---------------------------------------2025.10.11 ControlNet ë„ì…ë¶€ ìˆ˜ì •
            # íŒíŠ¸ ì¸ì½”ë”ë¥¼ ê±°ì¹˜ë©´ ì‹œì‘ ì±„ë„ì€ dims[0](=64)ë¡œ ê³ ì •
            c_curr = dims[0]
            #---------------------------------------2025.10.11 ControlNet ë„ì…ë¶€ ìˆ˜ì •
            for ind, (_din, dout) in enumerate(in_out):
                is_last = ind >= (num_resolutions - 1)
                if not is_last:
                    self.control_cond_downsamplers.append(Downsample(c_curr, dout))
                else:
                    self.control_cond_downsamplers.append(nn.Conv2d(c_curr, dout, 3, padding=1))
                c_curr = dout
            self.control_cond_mid = nn.Identity()  # ë¯¸ë“œ í•´ìƒë„ëŠ” ìœ„ì—ì„œ ë§ì¶°ì ¸ ìˆìŒ

            # ì—… ê²½ë¡œìš© ì—…ìƒ˜í”ŒëŸ¬(ì˜µì…˜)
            if self.control_inject_up:
                self.control_cond_upsamplers = ModuleList([])
                rev_in_out = list(reversed(in_out))
                for ind, (din, dout) in enumerate(rev_in_out):
                    is_last = ind == (len(rev_in_out) - 1)
                    if not is_last:
                        self.control_cond_upsamplers.append(Upsample(c_curr, din))
                    else:
                        self.control_cond_upsamplers.append(nn.Conv2d(c_curr, din, 3, padding=1))
                    c_curr = din

            # (B) trainable copy (down & mid & (opt) up) + zero-conv(pre/post)
            self.ctrl_down_pre = ModuleList([])     # pre zero-conv(cond -> dim_in)
            self.ctrl_down_copy = ModuleList([])    # trainable copy blocks
            self.ctrl_down_post = ModuleList([])    # post zero-conv(output -> dim_in), yì— ë”í•¨

            for ind, ((dim_in, dim_out), layer_full_attn, layer_attn_heads, layer_attn_dim_head) in enumerate(
                zip(in_out, full_attn, attn_heads, attn_dim_head)
            ):
                if not self.control_inject_down:
                    break
                attn_klass = FullAttention if layer_full_attn else LinearAttention

                # pre zero-conv: cond featureë¥¼ dim_inìœ¼ë¡œ (ì´ë¯¸ ì±„ë„ ì¼ì¹˜í•˜ì§€ë§Œ zero-convë¡œ ê²Œì´íŠ¸)
                pre = nn.Conv2d(dim_in, dim_in, 1, bias=True)
                nn.init.zeros_(pre.weight); nn.init.zeros_(pre.bias)
                self.ctrl_down_pre.append(pre)

                # trainable copy: ì› ë¸”ë¡ ë³µì œ (res1, res2, attn) - downsampleì€ ë³µì œ X
                locked_res1, locked_res2, locked_attn, _locked_down = self.downs[ind]
                copy_res1 = copy.deepcopy(locked_res1)
                copy_res2 = copy.deepcopy(locked_res2)
                copy_attn = copy.deepcopy(locked_attn)
                self.ctrl_down_copy.append(ModuleList([copy_res1, copy_res2, copy_attn]))

                # post zero-conv: copy ì¶œë ¥ ì±„ë„ì„ dim_inìœ¼ë¡œ ì •ë ¬í•´ì„œ ì”ì°¨ ìƒì„±
                post = nn.Conv2d(dim_in, dim_in, 1, bias=True)
                nn.init.zeros_(post.weight); nn.init.zeros_(post.bias)
                self.ctrl_down_post.append(post)

            # mid
            if self.control_inject_mid:
                self.ctrl_mid_pre  = nn.Conv2d(mid_dim, mid_dim, 1, bias=True)
                self.ctrl_mid_post = nn.Conv2d(mid_dim, mid_dim, 1, bias=True)
                nn.init.zeros_(self.ctrl_mid_pre.weight);  nn.init.zeros_(self.ctrl_mid_pre.bias)
                nn.init.zeros_(self.ctrl_mid_post.weight); nn.init.zeros_(self.ctrl_mid_post.bias)

                self.ctrl_mid_copy = ModuleList([
                    copy.deepcopy(self.mid_block1),
                    copy.deepcopy(self.mid_attn),
                    copy.deepcopy(self.mid_block2)
                ])
            else:
                self.ctrl_mid_pre = self.ctrl_mid_post = None
                self.ctrl_mid_copy = None

            # up (ì˜µì…˜)
            if self.control_inject_up:
                self.ctrl_up_pre  = ModuleList([])
                self.ctrl_up_copy = ModuleList([])
                self.ctrl_up_post = ModuleList([])

                rev_in_out = list(reversed(in_out))
                for ind, ((dim_in, dim_out), layer_full_attn, layer_attn_heads, layer_attn_dim_head) in enumerate(
                    zip(reversed(in_out), reversed(full_attn), reversed(attn_heads), reversed(attn_dim_head))
                ):
                    attn_klass = FullAttention if layer_full_attn else LinearAttention

                    # upì—ì„œëŠ” cat í›„ ì±„ë„ì´ (dim_out + dim_in)
                    pre = nn.Conv2d(dim_out + dim_in, dim_out + dim_in, 1, bias=True)
                    nn.init.zeros_(pre.weight); nn.init.zeros_(pre.bias)
                    self.ctrl_up_pre.append(pre)

                    locked_res1, locked_res2, locked_attn, _locked_up = self.ups[ind]
                    copy_res1 = copy.deepcopy(locked_res1)
                    copy_res2 = copy.deepcopy(locked_res2)
                    copy_attn = copy.deepcopy(locked_attn)
                    self.ctrl_up_copy.append(ModuleList([copy_res1, copy_res2, copy_attn]))

                    post = nn.Conv2d(dim_out, dim_out, 1, bias=True)  # block2 ì¶œë ¥ ì±„ë„ = dim_out
                    nn.init.zeros_(post.weight); nn.init.zeros_(post.bias)
                    self.ctrl_up_post.append(post)

            # (C) locked ë³¸ì²´ ë™ê²° ì˜µì…˜
            if self.freeze_locked:
                for p in self.downs.parameters(): p.requires_grad = False
                self.mid_block1.requires_grad_(False)
                for p in self.mid_attn.parameters(): p.requires_grad = False
                self.mid_block2.requires_grad_(False)
                for p in self.ups.parameters(): p.requires_grad = False
                self.final_res_block.requires_grad_(False)
                self.final_conv.requires_grad_(False)

    @property
    def downsample_factor(self):
        return 2 ** (len(self.downs) - 1)

    # -------------------------------------------------20251010 ìˆ˜ì • controlnet update (forwardì— cond ë° Control íë¦„ ë°˜ì˜)
    def forward(self, x, time, x_self_cond = None, cond = None):
        assert all([divisible_by(d, self.downsample_factor) for d in x.shape[-2:]]), f'your input dimensions {x.shape[-2:]} need to be divisible by {self.downsample_factor}, given the unet'

        if self.self_condition:
            x_self_cond = default(x_self_cond, lambda: torch.zeros_like(x))
            x = torch.cat((x_self_cond, x), dim = 1)

        x = self.init_conv(x)
        r = x.clone()

        t = self.time_mlp(time)

        # -------- condë¥¼ í˜„ì¬ í•´ìƒë„ë¡œ ë§ì¶°ê°€ë©° ë³´ê´€ (down ê²½ë¡œ ê¸°ì¤€)
        cond_feats = []
        if self.use_control and (cond is not None):
            #---------------------------------------2025.10.11 ControlNet ë„ì…ë¶€ ìˆ˜ì •
            # ì…ë ¥ cond(Kì±„ë„)ë¥¼ hint encoderë¡œ í†µê³¼ì‹œì¼œ ì›í•´ìƒë„ì—ì„œ 64ì±„ë„ë¡œ ì •ë ¬
            h_c = self.control_hint_encoder(cond)   # (B, 64, H, W)
            cond_feats.append(h_c)                  # stage 0 (dim_in = 64)

            # ì´í›„ ê° down-stageì˜ dim_inê³¼ í•´ìƒë„ì— ë§ë„ë¡ ìˆœì°¨ ë‹¤ìš´/ì±„ë„ ì •ë ¬
            for sampler in self.control_cond_downsamplers:
                h_c = sampler(h_c)                  # (B, 64/128/256/..., H/2^k, W/2^k)
                cond_feats.append(h_c)              # stage 1..N
            cond_mid = h_c                           # mid í•´ìƒë„ cond (dim = mid_dim)
            #---------------------------------------2025.10.11 ControlNet ë„ì…ë¶€ ìˆ˜ì •

        h = []

        # 1) DOWN
        for i, (block1, block2, attn, downsample) in enumerate(self.downs):
            # locked path: ì…ë ¥ xë¡œ ê·¸ëŒ€ë¡œ ì§„í–‰
            # trainable copy path: (ì„ íƒ) pre-zero(conv(cond))ë¥¼ ì…ë ¥ì—ë§Œ ë”í•´ ë³„ë„ ê²½ë¡œë¡œ ì²˜ë¦¬
            if self.use_control and self.control_inject_down and (cond is not None):
                x_ctrl_in = x + self.ctrl_down_pre[i](cond_feats[i])
                c1, c2, cattn = self.ctrl_down_copy[i]
                x_ctrl = c1(x_ctrl_in, t)
                x_ctrl = c2(x_ctrl, t)
                x_ctrl = cattn(x_ctrl) + x_ctrl
                # post zero-convë¡œ ì •ë ¬ í›„ locked ì¶œë ¥(y)ì— ì”ì°¨ë¡œ í•©ì‚° (ì›¨ì´íŠ¸X, í™œì„±ê°’O)
                ctrl_residual = self.ctrl_down_post[i](x_ctrl)
            else:
                ctrl_residual = None

            # locked ê²½ë¡œ ì‹¤ì œ ê³„ì‚°
            x = block1(x, t)
            h.append(x)

            x = block2(x, t)
            x = attn(x) + x

            # trainable copyì˜ post-zero ì¶œë ¥ì„ ì—¬ê¸°ì„œ í•©ì‚°
            if ctrl_residual is not None:
                x = x + ctrl_residual

            h.append(x)
            x = downsample(x)

        # 2) MID
        if self.use_control and self.control_inject_mid and (cond is not None):
            x_ctrl_mid_in = x + self.ctrl_mid_pre(cond_mid)     # pre zero-conv(cond -> mid_dim), trainable ì…ë ¥ì—ë§Œ
            m1, mattn, m2 = self.ctrl_mid_copy
            x_ctrl_mid = m1(x_ctrl_mid_in, t)
            x_ctrl_mid = mattn(x_ctrl_mid) + x_ctrl_mid
            x_ctrl_mid = m2(x_ctrl_mid, t)
            ctrl_mid_residual = self.ctrl_mid_post(x_ctrl_mid)  # post zero-conv
        else:
            ctrl_mid_residual = None

        x = self.mid_block1(x, t)
        x = self.mid_attn(x) + x
        x = self.mid_block2(x, t)

        if ctrl_mid_residual is not None:
            x = x + ctrl_mid_residual

        # (ì˜µì…˜) cond ì—…í•´ìƒë„ ì¤€ë¹„
        if self.use_control and self.control_inject_up and (cond is not None):
            h_c = cond_mid
            cond_up_feats = []
            for upsampler in self.control_cond_upsamplers:
                cond_up_feats.append(h_c)
                h_c = upsampler(h_c)

        # 3) UP
        for i, (block1, block2, attn, upsample) in enumerate(self.ups):
            x = torch.cat((x, h.pop()), dim = 1)

            if self.use_control and self.control_inject_up and (cond is not None):
                x_ctrl_in = x + self.ctrl_up_pre[i](cond_up_feats[i])
                u1, u2, uattn = self.ctrl_up_copy[i]
                x_ctrl = u1(x_ctrl_in, t)
                # upì˜ ë‘ ë²ˆì§¸ ë¸”ë¡ì€ concat í›„ì— ë“¤ì–´ê°€ë¯€ë¡œ ì ì‹œ ë’¤ì— copyë„ ë™ì¼í•œ ìˆœì„œë¥¼ ë§ì¶˜ë‹¤
                x = block1(x, t)

                # concat ë‘ ë²ˆì§¸ skip
                x = torch.cat((x, h.pop()), dim = 1)
                # trainable copyë„ ë™ì¼í•˜ê²Œ ë‘ ë²ˆì§¸ ì…ë ¥ì„ êµ¬ì„±í•´ì•¼ í•˜ì§€ë§Œ,
                # ê°„ë‹¨íˆëŠ” u2ë¥¼ x_ctrlì— ë°”ë¡œ ì ìš©(í‘œí˜„ë ¥ì€ ì¶©ë¶„)
                x_ctrl = u2(x_ctrl, t)
                x_ctrl = uattn(x_ctrl) + x_ctrl
                ctrl_up_residual = self.ctrl_up_post[i](x_ctrl)

                x = block2(x, t)
                x = attn(x) + x
                x = x + ctrl_up_residual
            else:
                x = block1(x, t)
                x = torch.cat((x, h.pop()), dim = 1)
                x = block2(x, t)
                x = attn(x) + x

            x = upsample(x)

        x = torch.cat((x, r), dim = 1)

        x = self.final_res_block(x, t)
        return self.final_conv(x)



# gaussian diffusion trainer class

# gaussian diffusion trainer class

def extract(a, t, x_shape):
    b, *_ = t.shape
    out = a.gather(-1, t)
    return out.reshape(b, *((1,) * (len(x_shape) - 1))) # (b, 1, 1, 1) ë°°ì¹˜ë§ˆë‹¤ ë‹¤ë¥¸ ìŠ¤ì¹¼ë¼ê°€ ì „ì²´ ê³µê°„(C,H,W)ì— ì ìš©

def linear_beta_schedule(timesteps):
    """
    linear schedule, proposed in original ddpm paper
    """
    scale = 1000 / timesteps
    beta_start = scale * 0.0001
    beta_end = scale * 0.02
    return torch.linspace(beta_start, beta_end, timesteps, dtype = torch.float64)

def cosine_beta_schedule(timesteps, s = 0.008):
    """
    cosine schedule
    as proposed in https://openreview.net/forum?id=-NEXDKk8gZ
    """
    steps = timesteps + 1 # 
    t = torch.linspace(0, timesteps, steps, dtype = torch.float64) / timesteps # 0~1 ê· ë“±ë¶„í¬
    alphas_cumprod = torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** 2  # 0~1 ì½”ì‚¬ì¸ ê³¡ì„ 
    alphas_cumprod = alphas_cumprod / alphas_cumprod[0] # 1ë¡œ ì •ê·œí™”
    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])  # 1 - (Î±_t / Î±_(t-1)) = Î²_t
    return torch.clip(betas, 0, 0.999)  # 0~0.999 ì‚¬ì´ë¡œ í´ë¨í•‘

def sigmoid_beta_schedule(timesteps, start = -3, end = 3, tau = 1, clamp_min = 1e-5):
    """
    sigmoid schedule
    proposed in https://arxiv.org/abs/2212.11972 - Figure 8
    better for images > 64x64, when used during training
    """
    steps = timesteps + 1
    t = torch.linspace(0, timesteps, steps, dtype = torch.float64) / timesteps
    v_start = torch.tensor(start / tau).sigmoid() # start =-3, tau=1 â†’ 0.0474
    v_end = torch.tensor(end / tau).sigmoid()
    alphas_cumprod = (-((t * (end - start) + start) / tau).sigmoid() + v_end) / (v_end - v_start)
    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]
    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])
    return torch.clip(betas, 0, 0.999)

class GaussianDiffusion(Module):
    def __init__(
        self,
        model,
        *,
        image_size,
        timesteps = 1000,
        sampling_timesteps = None,
        objective = 'pred_v',
        beta_schedule = 'sigmoid',
        schedule_fn_kwargs = dict(),
        ddim_sampling_eta = 0.,
        auto_normalize = True,
        offset_noise_strength = 0.,  # https://www.crosslabs.org/blog/diffusion-with-offset-noise
        min_snr_loss_weight = False, # https://arxiv.org/abs/2303.09556
        min_snr_gamma = 5,
        immiscible = False,
        # â–¼ ìƒˆë¡œ ì¶”ê°€: MIP ì†ì‹¤ ê´€ë ¨
        mip_options=None,     # <- MIPlossê°€ í•„ìš”ë¡œ í•˜ëŠ” ì„¤ì •ë§Œ ë‹´ì€ ê°ì²´/ë”•íŠ¸
        lambda_mip=1.0,       # <- MIP ì†ì‹¤ ê°€ì¤‘ì¹˜
    ):
        super().__init__()
        assert not (type(self) == GaussianDiffusion and model.channels != model.out_dim)
        assert not hasattr(model, 'random_or_learned_sinusoidal_cond') or not model.random_or_learned_sinusoidal_cond

        self.model = model

        self.channels = self.model.channels # 3
        self.self_condition = self.model.self_condition # False

        if isinstance(image_size, int):
            image_size = (image_size, image_size)
        assert isinstance(image_size, (tuple, list)) and len(image_size) == 2, 'image size must be a integer or a tuple/list of two integers'   # (H, W) 
        self.image_size = image_size    # (H, W) 320, 320

        self.objective = objective # 'pred_v'

        assert objective in {'pred_noise', 'pred_x0', 'pred_v'}, 'objective must be either pred_noise (predict noise) or pred_x0 (predict image start) or pred_v (predict v [v-parameterization as defined in appendix D of progressive distillation paper, used in imagen-video successfully])'

        if beta_schedule == 'linear':
            beta_schedule_fn = linear_beta_schedule
        elif beta_schedule == 'cosine':
            beta_schedule_fn = cosine_beta_schedule
        elif beta_schedule == 'sigmoid':
            beta_schedule_fn = sigmoid_beta_schedule
        else:
            raise ValueError(f'unknown beta schedule {beta_schedule}')

        betas = beta_schedule_fn(timesteps, **schedule_fn_kwargs)

        alphas = 1. - betas
        alphas_cumprod = torch.cumprod(alphas, dim=0)
        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value = 1.)

        timesteps, = betas.shape
        self.num_timesteps = int(timesteps)



        # sampling related parameters

        self.sampling_timesteps = default(sampling_timesteps, timesteps) # default num sampling timesteps to number of timesteps at training

        assert self.sampling_timesteps <= timesteps # True
        self.is_ddim_sampling = self.sampling_timesteps < timesteps # True
        self.ddim_sampling_eta = ddim_sampling_eta  # 0.0

        # helper function to register buffer from float64 to float32

        register_buffer = lambda name, val: self.register_buffer(name, val.to(torch.float32))

        register_buffer('betas', betas)
        register_buffer('alphas_cumprod', alphas_cumprod)
        register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)

        # calculations for diffusion q(x_t | x_{t-1}) and others

        register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))
        register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1. - alphas_cumprod))
        register_buffer('log_one_minus_alphas_cumprod', torch.log(1. - alphas_cumprod))
        register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1. / alphas_cumprod))
        register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1. / alphas_cumprod - 1))

        # calculations for posterior q(x_{t-1} | x_t, x_0)

        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod) # 

        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)

        register_buffer('posterior_variance', posterior_variance)

        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain

        register_buffer('posterior_log_variance_clipped', torch.log(posterior_variance.clamp(min =1e-20)))
        register_buffer('posterior_mean_coef1', betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod))
        register_buffer('posterior_mean_coef2', (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod))

        # mip loss 2025-10-07
        self.lambda_mip = float(lambda_mip)
        self.mip_loss = None   # ì¼ë‹¨ Noneìœ¼ë¡œ ë‘ê³ , ë²„í¼ ë“±ë¡ í›„ ë””ë°”ì´ìŠ¤ê°€ ì •í•´ì§€
        self.awl = None        # AWLë„ ê¸°ë³¸ Noneìœ¼ë¡œ
        # (ì¤‘ëµ) betas, alphas ë“± register_buffer ëë‚œ ë’¤ì— ìƒì„±í•´ì•¼ self.device ì‚¬ìš© ê°€ëŠ¥
        if mip_options is not None:
            self._init_mip_loss(mip_options)


        # immiscible diffusion

        self.immiscible = immiscible

        # offset noise strength - in blogpost, they claimed 0.1 was ideal

        self.offset_noise_strength = offset_noise_strength

        # derive loss weight
        # snr - signal noise ratio

        snr = alphas_cumprod / (1 - alphas_cumprod)

        # https://arxiv.org/abs/2303.09556

        maybe_clipped_snr = snr.clone()
        if min_snr_loss_weight:
            maybe_clipped_snr.clamp_(max = min_snr_gamma)

        if objective == 'pred_noise':
            register_buffer('loss_weight', maybe_clipped_snr / snr)
        elif objective == 'pred_x0':
            register_buffer('loss_weight', maybe_clipped_snr)
        elif objective == 'pred_v':
            register_buffer('loss_weight', maybe_clipped_snr / (snr + 1))

        # auto-normalization of data [0, 1] -> [-1, 1] - can turn off by setting it to be False

        self.normalize = normalize_to_neg_one_to_one if auto_normalize else identity
        self.unnormalize = unnormalize_to_zero_to_one if auto_normalize else identity

    def _init_mip_loss(self, mip_options):
        """
        MIP ì†ì‹¤, AWL, ì‹œê·¸ë§ˆ ìŠ¤ì¼€ì¤„ ì´ˆê¸°í™”
        - dict/ê°ì²´ ëª¨ë‘ ì§€ì›
        - num_sliceëŠ” ì±„ë„ ìˆ˜ì™€ ë™ê¸°í™”
        """
        def _get(obj, path, default=None):
            cur = obj
            for key in path.split('.'):
                if isinstance(cur, dict):
                    cur = cur.get(key, None)
                else:
                    cur = getattr(cur, key, None)
                if cur is None:
                    return default
            return cur

        temp = _get(mip_options, 'mip_loss.temp', 1.0)
        num_slice = _get(mip_options, 'data.use_slice', self.channels)

        # MIPlossê°€ ì†ì„± ì ‘ê·¼ì„ ê¸°ëŒ€í•˜ë¯€ë¡œ ì–•ì€ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ êµ¬ì„±
        class _Opt: pass
        opt = _Opt()
        opt.mip_loss = _Opt()
        opt.mip_loss.temp = float(temp)
        opt.data = _Opt()
        opt.data.use_slice = int(num_slice if num_slice is not None else self.channels)

        self.mip_loss = MIPloss(opt).to(self.device)
        # ì±„ë„ ìˆ˜ì™€ ê°•ì œ ë™ê¸°í™” (ë¶ˆì¼ì¹˜ ì‹œ ìŠ¬ë¼ì´ì‹± ì´ìƒ ë°©ì§€)
        if getattr(self.mip_loss, 'num_slice', None) != self.channels:
            self.mip_loss.num_slice = self.channels

        # ê²°í•© ì†ì‹¤ 2ê°œ(base, mip)ì— ë§ì¶° íŒŒë¼ë¯¸í„° ê°œìˆ˜ ì§€ì •
        self.awl = AutomaticWeightedLoss(num=4).to(self.device)

        # timestepë³„ sigma ìŠ¤ì¼€ì¤„(ì—¬ê¸°ì„œëŠ” 0ìœ¼ë¡œ ì´ˆê¸°í™” â€“ í•„ìš” ì‹œ ë°”ê¾¸ë©´ ë¨)
        # sigma_sched = torch.zeros(self.num_timesteps, device=self.device, dtype=torch.float32) # sigma_sched = í•­ìƒ 0ìœ¼ë¡œ ë˜ì–´ìˆëŠ” ìƒíƒœì„ 2025-10-09
        # self.register_buffer('some_sigma_sched', sigma_sched)
        T = self.num_timesteps # 1000
        t = torch.arange(T, device=self.device, dtype=torch.float32)
        m = t / (T - 1)
        delta_t = 2.0 * (m - m * m)          # ë…¼ë¬¸ì˜ Î´_t
        self.register_buffer('some_sigma_sched', delta_t)

    @property
    def device(self):
        return self.betas.device

    def predict_start_from_noise(self, x_t, t, noise): # x_t: noisy image, t: time step, noise: predicted noise/ ë…¸ì´ì¦ˆê°€ ì„ì¸ ìƒíƒœ x_tì™€ í•´ë‹¹ ì‹œì ì˜ ë…¸ì´ì¦ˆ Îµê°€ ìˆìœ¼ë©´, ê¹¨ë—í•œ ì›ë³¸ xâ‚€ë¥¼ ë³µì›.
        return (
            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -
            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise
        )

    def predict_noise_from_start(self, x_t, t, x0): # x_t: noisy image, t: time step, x0: predicted original image/ ë…¸ì´ì¦ˆê°€ ì„ì¸ ìƒíƒœ x_tì™€ í•´ë‹¹ ì‹œì ì˜ ì›ë³¸ ì´ë¯¸ì§€ xâ‚€ê°€ ìˆìœ¼ë©´, ê·¸ ì‹œì ì˜ ë…¸ì´ì¦ˆ Îµë¥¼ ì˜ˆì¸¡.
        return (
            (extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - x0) / \
            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)
        )

    def predict_v(self, x_start, t, noise):
        return (
            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * noise -
            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * x_start
        )

    def predict_start_from_v(self, x_t, t, v): # vì—ì„œ ë‹¤ì‹œ xâ‚€ë¥¼ ë³µì›í•˜ëŠ” ê³µì‹ì…ë‹ˆë‹¤. (pred_vì¼ ë•Œ ëª¨ë¸ ì¶œë ¥ â†’ xâ‚€ë¡œ ë°”ê¿” ì“°ë ¤ë©´ í•„ìš”)
        return (
            extract(self.sqrt_alphas_cumprod, t, x_t.shape) * x_t -
            extract(self.sqrt_one_minus_alphas_cumprod, t, x_t.shape) * v
        )

    def q_posterior(self, x_start, x_t, t): # DDPM ìƒ˜í”ŒëŸ¬ë¼ë©´ q_posteriorê°€ ë§¤ ìŠ¤í… í•„ìš”
        posterior_mean = (
            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start +
            extract(self.posterior_mean_coef2, t, x_t.shape) * x_t
        )
        posterior_variance = extract(self.posterior_variance, t, x_t.shape)
        posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)
        return posterior_mean, posterior_variance, posterior_log_variance_clipped

    # -------------------------------------------------20251010 ìˆ˜ì • controlnet update (cond ì „ë‹¬ ê²½ë¡œ ì¶”ê°€)
    def model_predictions(self, x, t, x_self_cond = None, clip_x_start = False, rederive_pred_noise = False, *, cond=None):
        model_output = self.model(x, t, x_self_cond, cond=cond)  # â† cond ì „ë‹¬
        maybe_clip = partial(torch.clamp, min = -1., max = 1.) if clip_x_start else identity

        if self.objective == 'pred_noise':
            pred_noise = model_output
            x_start = self.predict_start_from_noise(x, t, pred_noise)
            x_start = maybe_clip(x_start)

            if clip_x_start and rederive_pred_noise:
                pred_noise = self.predict_noise_from_start(x, t, x_start)

        elif self.objective == 'pred_x0':
            x_start = model_output
            x_start = maybe_clip(x_start)
            pred_noise = self.predict_noise_from_start(x, t, x_start)

        elif self.objective == 'pred_v':
            v = model_output
            x_start = self.predict_start_from_v(x, t, v)
            x_start = maybe_clip(x_start)
            pred_noise = self.predict_noise_from_start(x, t, x_start)

        return ModelPrediction(pred_noise, x_start)

    # -------------------------------------------------20251010 ìˆ˜ì • controlnet update (cond ì „ë‹¬ ê²½ë¡œ ì¶”ê°€)
    def p_mean_variance(self, x, t, x_self_cond = None, clip_denoised = True, *, cond=None): # í˜„ì¬ì‹œì  X_tì—ì„œ ì´ì „ì‹œì  X_{t-1}ë¡œ ë„˜ì–´ê°€ê¸° ìœ„í•œ í‰ê· ê³¼ ë¶„ì‚° ì¦‰ ìƒ˜í”Œë§í•  ê°€ìš°ì‹œì•ˆ íŒŒë¼ë¯¸í„°ë¥¼ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤.
        preds = self.model_predictions(x, t, x_self_cond, clip_x_start=False, rederive_pred_noise=False, cond=cond) # ModelPrediction(pred_noise, pred_x_start)
        x_start = preds.pred_x_start

        if clip_denoised:
            x_start.clamp_(-1., 1.)

        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start = x_start, x_t = x, t = t)
        return model_mean, posterior_variance, posterior_log_variance, x_start

    # -------------------------------------------------20251010 ìˆ˜ì • controlnet update (cond ì „ë‹¬ ê²½ë¡œ ì¶”ê°€)
    @torch.inference_mode()
    def p_sample(self, x, t: int, x_self_cond = None, *, cond=None):
        b, *_, device = *x.shape, self.device
        batched_times = torch.full((b,), t, device = device, dtype = torch.long)
        model_mean, _, model_log_variance, x_start = self.p_mean_variance(x = x, t = batched_times, x_self_cond = x_self_cond, clip_denoised = True, cond=cond)
        noise = torch.randn_like(x) if t > 0 else 0. # no noise if t == 0
        pred_img = model_mean + (0.5 * model_log_variance).exp() * noise
        return pred_img, x_start

    # -------------------------------------------------20251010 ìˆ˜ì • controlnet update (cond ì „ë‹¬ ê²½ë¡œ ì¶”ê°€)
    @torch.inference_mode()
    def p_sample_loop(self, shape, return_all_timesteps = False, *, cond=None):
        batch, device = shape[0], self.device

        img = torch.randn(shape, device = device)
        imgs = [img]

        x_start = None

        for t in tqdm(reversed(range(0, self.num_timesteps)), desc = 'sampling loop time step', total = self.num_timesteps):
            self_cond = x_start if self.self_condition else None
            img, x_start = self.p_sample(img, t, self_cond, cond=cond) # pred_img, pred_x_start
            imgs.append(img)

        ret = img if not return_all_timesteps else torch.stack(imgs, dim = 1) # return_all_timestepsì´ Trueë©´ (B, T, C, H, W) í…ì„œë¡œ ë°˜í™˜ / return_all_timestepsì´ Falseë©´ (B, C, H, W) í…ì„œë¡œ ë°˜í™˜

        ret = self.unnormalize(ret)
        return ret

    # -------------------------------------------------20251010 ìˆ˜ì • controlnet update (cond ì „ë‹¬ ê²½ë¡œ ì¶”ê°€)
    @torch.inference_mode()
    def ddim_sample(self, shape, return_all_timesteps = False, *, cond=None):
        batch, device, total_timesteps, sampling_timesteps, eta, objective = shape[0], self.device, self.num_timesteps, self.sampling_timesteps, self.ddim_sampling_eta, self.objective

        times = torch.linspace(-1, total_timesteps - 1, steps = sampling_timesteps + 1)   # [-1, 0, 1, 2, ..., T-1] when sampling_timesteps == total_timesteps
        times = list(reversed(times.int().tolist()))
        time_pairs = list(zip(times[:-1], times[1:])) # [(T-1, T-2), (T-2, T-3), ..., (1, 0), (0, -1)]

        img = torch.randn(shape, device = device)
        imgs = [img]

        x_start = None

        for time, time_next in tqdm(time_pairs, desc = 'sampling loop time step'):
            time_cond = torch.full((batch,), time, device = device, dtype = torch.long)
            self_cond = x_start if self.self_condition else None
            pred_noise, x_start, *_ = self.model_predictions(img, time_cond, self_cond, clip_x_start = True, rederive_pred_noise = True, cond=cond)

            if time_next < 0: # time_next == -1
                img = x_start
                imgs.append(img)
                continue

            alpha = self.alphas_cumprod[time]
            alpha_next = self.alphas_cumprod[time_next]

            sigma = eta * ((1 - alpha / alpha_next) * (1 - alpha_next) / (1 - alpha)).sqrt()
            c = (1 - alpha_next - sigma ** 2).sqrt()

            noise = torch.randn_like(img)

            img = x_start * alpha_next.sqrt() + \
                  c * pred_noise + \
                  sigma * noise

            imgs.append(img)

        ret = img if not return_all_timesteps else torch.stack(imgs, dim = 1)

        ret = self.unnormalize(ret)
        return ret

    # -------------------------------------------------20251010 ìˆ˜ì • controlnet update (cond ì „ë‹¬ ê²½ë¡œ ì¶”ê°€)
    @torch.inference_mode()
    def sample(self, batch_size = 16, return_all_timesteps = False, *, cond=None):
        (h, w), channels = self.image_size, self.channels
        sample_fn = self.p_sample_loop if not self.is_ddim_sampling else self.ddim_sample
        return sample_fn((batch_size, channels, h, w), return_all_timesteps = return_all_timesteps, cond=cond)

    @torch.inference_mode()
    def interpolate(self, x1, x2, t = None, lam = 0.5):
        b, *_, device = *x1.shape, x1.device
        t = default(t, self.num_timesteps - 1)

        assert x1.shape == x2.shape

        t_batched = torch.full((b,), t, device = device)
        xt1, xt2 = map(lambda x: self.q_sample(x, t = t_batched), (x1, x2))

        img = (1 - lam) * xt1 + lam * xt2

        x_start = None

        for i in tqdm(reversed(range(0, t)), desc = 'interpolation sample time step', total = t):
            self_cond = x_start if self.self_condition else None
            img, x_start = self.p_sample(img, i, self_cond)

        return img


    def noise_assignment(self, x_start, noise): # ì´ë¯¸ì§€ì— ê°€ì¥ ì˜ ë§ëŠ”(ê°€ê¹Œìš´) ë…¸ì´ì¦ˆâ€ë¥¼ ê²¹ì¹˜ì§€ ì•Šê²Œ ë°°ë¶„í•˜ëŠ” í•¨ìˆ˜ # ië²ˆì§¸ x_startì—ëŠ” ì–´ë–¤ noiseë¥¼ ë¶™ì´ë©´ ì „ì²´ ê±°ë¦¬ í•©ì´ ìµœì†Œê°€ ë˜ëŠ”ê°€?â€ë¥¼ **í—ê°€ë¦¬ì•ˆ ì•Œê³ ë¦¬ì¦˜**(ìµœì†Œ ë¹„ìš© ë§¤ì¹­)ìœ¼ë¡œ ì°¾ì•„ ì¸ë±ìŠ¤ ë§¤í•‘ì„ ëŒ
        x_start, noise = tuple(rearrange(t, 'b ... -> b (...)') for t in (x_start, noise)) # (B, C, H, W) â†’ (B, C*H*W) ë°°ì¹˜ ì°¨ì› bë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ì¶•ì„ ì „ë¶€ í¼ì³ì„œ 2Dë¡œ ë§Œë“­ë‹ˆë‹¤.
        dist = torch.cdist(x_start, noise) # (B, B) ê° ë°°ì¹˜ ìŒë§ˆë‹¤ì˜ ê±°ë¦¬/ í–‰ë ¬ ìœ í´ë¦¬ë“œ ê±°ë¦¬(ê¸°ë³¸ê°’)ë¡œ ëª¨ë“  ìŒì˜ ê±°ë¦¬ í–‰ë ¬ì„ ë§Œë“­ë‹ˆë‹¤. (í–‰ = ê° x_start ìƒ˜í”Œ, ì—´ = ê° noise ìƒ˜í”Œ) ì¦‰ dist[i, j]ëŠ” x_start[i]ì™€ noise[j]ì˜ ê±°ë¦¬.
        _, assign = linear_sum_assignment(dist.cpu())
        return torch.from_numpy(assign).to(dist.device)

    @autocast('cuda', enabled = False)
    def q_sample(self, x_start, t, noise = None):
        noise = default(noise, lambda: torch.randn_like(x_start))

        if self.immiscible:
            assign = self.noise_assignment(x_start, noise)  
            noise = noise[assign]

        return (
            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +
            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise
        )

    # def p_losses(self, x_start, t, noise = None, offset_noise_strength = None):
    # -------------------------------------------------20251010 ìˆ˜ì • controlnet update (cond ì¸ì ì¶”ê°€: í•™ìŠµ ì‹œ ì¡°ê±´ ì „ë‹¬)
    def p_losses(self, x_start, t, noise = None, offset_noise_strength = None, *, return_stats: bool = False, cond=None): # MIP lossë„ tqdm pbarë¡œ ë‚˜íƒ€ë‚´ë ¤ê³  ì¶”ê°€í•œê±°ì„
        b, c, h, w = x_start.shape

        noise = default(noise, lambda: torch.randn_like(x_start))

        # offset noise - https://www.crosslabs.org/blog/diffusion-with-offset-noise

        # ì´ë¯¸ì§€ ì „ì²´(ê³µê°„ ì „ì²´)ì— ê°™ì€ ì˜¤í”„ì…‹ì„ ë„£ì–´ ì±„ë„ ë°”ì´ì–´ìŠ¤ë¥¼ ëœë¤í•˜ê²Œ ì£¼ë©´ ë¶„í¬ ì»¤ë²„ë¦¬ì§€ê°€ ë„“ì–´ì§€ë©´ì„œ í›ˆë ¨ ì•ˆì •/í’ˆì§ˆì´ ì¢‹ì•„ì§€ëŠ” ë³´ê³ ê°€ ìˆìŒ
        offset_noise_strength = default(offset_noise_strength, self.offset_noise_strength) # 0.0
        
        if offset_noise_strength > 0.:
            offset_noise = torch.randn(x_start.shape[:2], device = self.device)
            noise += offset_noise_strength * rearrange(offset_noise, 'b c -> b c 1 1')

        # noise sample

        x = self.q_sample(x_start = x_start, t = t, noise = noise) # noisy image / ì •ë°©í–¥ ìƒ˜í”Œë§ìœ¼ë¡œ ë…¸ì´ì¦ˆê°€ ì„ì¸ ì´ë¯¸ì§€ x_të¥¼ ë§Œë“­ë‹ˆë‹¤.

        # if doing self-conditioning, 50% of the time, predict x_start from current set of times
        # and condition with unet with that
        # this technique will slow down training by 25%, but seems to lower FID significantly

        x_self_cond = None
        if self.self_condition and random() < 0.5:
            with torch.no_grad():
                # -------------------------------------------------20251010 ìˆ˜ì • controlnet update (cond ì „ë‹¬)
                x_self_cond = self.model_predictions(x, t, cond=cond).pred_x_start
                x_self_cond.detach_()

        # predict and take gradient step

        # -------------------------------------------------20251010 ìˆ˜ì • controlnet update (cond ì „ë‹¬)
        model_out = self.model(x, t, x_self_cond, cond=cond)  # êµ¬í˜„ ì„¤ì •ì— ë”°ë¼ Îµ / xâ‚€ / v ì¤‘ í•˜ë‚˜ë¥¼ ì§ì ‘ ì˜ˆì¸¡í•˜ë„ë¡ ì„¤ê³„, ì´ í•¨ìˆ˜ì—ì„  â€œëª¨ë¸ì´ ë‚¸ ê²ƒâ€ì„ model_outì´ë¼ ë‘ê³ , ê·¸ì— ë§ëŠ” ì •ë‹µ(target) ì„ ì•„ë˜ì—ì„œ ë§Œë“ ë‹¤

        if self.objective == 'pred_noise':
            target = noise
        elif self.objective == 'pred_x0':
            target = x_start
        elif self.objective == 'pred_v':
            v = self.predict_v(x_start, t, noise)
            target = v
        else:
            raise ValueError(f'unknown objective {self.objective}')

        # ################################################################################## 
        # loss = F.mse_loss(model_out, target, reduction = 'none')
        # loss = reduce(loss, 'b ... -> b', 'mean')

        # loss = loss * extract(self.loss_weight, t, loss.shape) # self.loss_weight = SNR ê¸°ë°˜ ì†ì‹¤ ê°€ì¤‘ì¹˜ (ë°°ì¹˜ë§ˆë‹¤ ë‹¤ë¥¸ ìŠ¤ì¹¼ë¼) ì ìš© # (B,) â†’ (B, 1, 1, 1)ë¡œ ë³€í™˜ë˜ì–´ ê° ë°°ì¹˜ì— ê³±í•´ì§ 
        # return loss.mean()
        # ################################################################################## ì›ë˜ ì—¬ê¸°ê¹Œì§€ì½”ë“œê°€ ë§ìŒ ì•„ë˜ëŠ” ì¶”ê°€í•¨
        
        # ===================== ğŸ‘‡ğŸ‘‡ğŸ‘‡ ìƒˆ ì˜µì…˜ ì¶”ê°€ ğŸ‘‡ğŸ‘‡ğŸ‘‡ ===================== 
        ### SeoSY 2025-07-24 (ê¸°ì¡´ ì†ì‹¤í•¨ìˆ˜ì—ì„œ MIP lossë¡œ ë³€ê²½)
        base_loss = F.mse_loss(model_out, target, reduction = 'none')
        base_loss = reduce(base_loss, 'b ... -> b', 'mean')
        base_loss = base_loss * extract(self.loss_weight, t, base_loss.shape)
        base_loss = base_loss.mean()

        # MIP ë¹„í™œì„± ì‹œì—ëŠ” ê¸°ë³¸ ì†ì‹¤ë§Œ ë°˜í™˜
        # ---------------------------------------------------------------------------2025-10-09 MIP lossë¥¼ pbarë¡œ ë‚˜íƒ€ë‚´ê¸° Seo ğŸ‘‡
        # if getattr(self, 'mip_loss', None) is None or getattr(self, 'awl', None) is None:
        #     return base_loss
        # ----- base_lossë§Œ ì“°ëŠ” ê²½ë¡œ ----- ì›ë˜ ìœ„ì— 2ì¤„ì´ì˜€ëŠ”ë° MIP loss ë„ pbarë¡œ ë‚˜íƒ€ë‚´ë ¤ê³  ì•„ë˜ ì¤„ë¡œ ë°”ë€œ
        if getattr(self, 'mip_loss', None) is None or getattr(self, 'awl', None) is None:
            if return_stats:
                stats = {'base': float(base_loss.detach().item()), 'mip': None, 'total': float(base_loss.detach().item())}
                return base_loss, stats
            return base_loss
        # ---------------------------------------------------------------------------------------2025-10-09 MIP lossë¥¼ pbarë¡œ ë‚˜íƒ€ë‚´ê¸° Seo ğŸ‘†

        ## pred_noise, pred_x0, pred_v ê³µí†µ: ëª¨ë¸ ì¶œë ¥ â†’ x0_hat êµ¬í•˜ê¸°, no gradë¥¼ í†µí•´ì„œ grad ì—°ê²° ì°¨ë‹¨
        if self.objective in ('pred_noise', 'pred_v'):
            with torch.no_grad():
                a_bar_sqrt     = extract(self.sqrt_alphas_cumprod, t, x.shape)
                one_minus_sqrt = extract(self.sqrt_one_minus_alphas_cumprod, t, x.shape)

        if self.objective == 'pred_noise':
            x0_hat = (x - one_minus_sqrt * model_out) / a_bar_sqrt
        elif self.objective == 'pred_x0':
            x0_hat = model_out                  # grad ì—°ê²° ìœ ì§€
        elif self.objective == 'pred_v':
            x0_hat = a_bar_sqrt * x - one_minus_sqrt * model_out
        ## ìœ„ ì½”ë“œëŠ” MIP lossìš© x0_hat êµ¬í•˜ëŠ” ì½”ë“œ x0hatì€ (B, C, H, W) í…ì„œì´ë©° ê° ë°°ì¹˜ì— ëŒ€í•œ ëª¨ë¸ì˜ xâ‚€ ì˜ˆì¸¡ì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.

        # í˜•íƒœí™•ì¸ìš© print(x0_hat.shape) # (B, C, H, W)
        # í•„ìš”í•˜ë‹¤ë©´ view/permuteë¡œ ìŠ¬ë¼ì´ìŠ¤ ì¶•ì´ dim=1ì— ì˜¤ë„ë¡ ë°”ê¿”ì„œ ì‚¬ìš©
        # [B, D, H, W] ë¼ë©´ Dê°€ ì±„ë„ì´ì ìŠ¬ë¼ì´ìŠ¤ ì¶•
        assert x0_hat.dim() == 4 and x0_hat.size(0) == x_start.size(0) 

        # MIPlossì˜ slice ìˆ˜ë¥¼ ì±„ë„ì— ë§ì¶° ê°•ì œ ë™ê¸°í™”(ì˜µì…˜ ë¶ˆì¼ì¹˜ ëŒ€ë¹„)
        if getattr(self.mip_loss, 'num_slice', None) != x0_hat.shape[1]:
            self.mip_loss.num_slice = x0_hat.shape[1]

        # MIP loss
        mip_loss = self.mip_loss(x0_hat, x_start)

        # ===================== ì—¬ê¸°ë¶€í„° ì¶”ê°€: í˜ˆê´€ê°€ì¤‘ L2 =====================
        # GT TOFê°€ ë°ì€ ê³³ì¼ìˆ˜ë¡ ì¤‘ìš”í•˜ê²Œ ë³´ë„ë¡ weight map ìƒì„±
        with torch.no_grad():
            tau   = 0.15   # ë°ê¸° ê¸°ì¤€ (ë°ì´í„° ë³´ê³  ì¡°ì ˆ)
            sharp = 5.0   # ê²½ì‚¬
            m = torch.sigmoid((x_start - tau) * sharp)    # (B, C, H, W), í˜ˆê´€ì¼ìˆ˜ë¡ 1ì— ê°€ê¹Œì›€

        lambda_vessel = 3.0
        weight_map = 1.0 + lambda_vessel * m
        vessel_weighted_l2 = (weight_map * (x0_hat - x_start).pow(2)).mean()
        # ================================================================

        # ===================== Edge-aware ì†ì‹¤ ì¶”ê°€ =====================
        def sobel_filter(x):
            # x: (B, C, H, W)  â€” Cì±„ë„ ê°ê°ì— ë™ì¼ Sobel ì ìš©
            B, C, H, W = x.shape

            kx = torch.tensor(
                [[[-1, 0, 1],
                  [-2, 0, 2],
                  [-1, 0, 1]]],
                dtype=x.dtype,
                device=x.device
            ).unsqueeze(0)          # [1,1,3,3]

            ky = torch.tensor(
                [[[-1, -2, -1],
                  [ 0,  0,  0],
                  [ 1,  2,  1]]],
                dtype=x.dtype,
                device=x.device
            ).unsqueeze(0)          # [1,1,3,3]

            # ê° ì±„ë„ë³„ë¡œ ê°™ì€ ì»¤ë„ì„ ì“°ê¸° ìœ„í•´ Cê°œë¡œ expand í›„ groups=C ì‚¬ìš©
            kx = kx.expand(C, 1, 3, 3)     # [C,1,3,3]
            ky = ky.expand(C, 1, 3, 3)     # [C,1,3,3]

            gx = F.conv2d(x, kx, padding=1, groups=C)   # (B,C,H,W)
            gy = F.conv2d(x, ky, padding=1, groups=C)   # (B,C,H,W)

            return torch.sqrt(gx ** 2 + gy ** 2 + 1e-8)

        with torch.no_grad():
            gt_edge = sobel_filter(x_start)
        pred_edge = sobel_filter(x0_hat)
        loss_edge = F.l1_loss(pred_edge, gt_edge)
        # ================================================================


        # 5) ê²°í•©
        # (a) ê°„ë‹¨ ê°€ì¤‘í•©
        # lam_mip = getattr(self, 'lambda_mip', 1.0)   # í•˜ì´í¼íŒŒë¼ë¯¸í„°
        # total_loss = base_loss + lam_mip * mip_loss

        # (b) AutomaticWeightedLossë¡œ ë¶ˆí™•ì‹¤ì„± ê¸°ë°˜ ê°€ì¤‘(ì›í•˜ë©´)
        awl_losses = [base_loss, mip_loss]

        # ---------------------------------------------------------------------------2025-10-09 MIP lossë¥¼ ë°°ì¹˜ë§ˆë‹¤ ë‹¤ë¥¸ delta të¡œ ì ìš©í•˜ê¸° mean ì•Ší•˜ê³  Seo ğŸ‘‡
        # sigma_t = extract(self.some_sigma_sched, t, x.shape).mean() # sigma_tëŠ” í…ì„œ ìŠ¤ì¹¼ë¼ë¡œ(ë¸Œë¡œë“œìºìŠ¤íŠ¸/shape ì´ìŠˆ ë°©ì§€). timestepë³„ ë²„í¼ì—ì„œ ì¶”ì¶œ í›„ í‰ê· (ì´ëŸ¬ë©´ ë°°ì¹˜ 8ë§Œí¼ ).
        # total_loss = self.awl(awl_losses, sigma_t=sigma_t)
        ### ê·¸ëƒ¥ ì•„ë˜ êº¼ ë§ê³  ìœ„ì— ë‘ì¤„ì“°ëŠ”ê²Œ ì†í¸í•¨
        # ë³€ê²½ëœ ë¶€ë¶„: ë°°ì¹˜ í‰ê· ì´ ì•„ë‹Œ ìƒ˜í”Œë³„ sigma_t (Î´_t) ë²¡í„° ì‚¬ìš© â¬‡ ğŸ‘‡  
        B = x.size(0)       # ìƒ˜í”Œë³„ sigma (Î´_t) ë²¡í„° [B]
        sigma_vec = extract(self.some_sigma_sched, t, (B, 1, 1, 1)).squeeze(-1).squeeze(-1).squeeze(-1)  # [B]

        # AWL íŒŒë¼ë¯¸í„°ë¡œ ì¡°í•©ì„ í˜¸ì¶œë¶€ì—ì„œ ì§ì ‘ ê³„ì‚° (AWL ì½”ë“œëŠ” ì†ëŒ€ì§€ ì•ŠìŒ)
        theta0_sq = self.awl.params[0] ** 2   # baseìš©
        theta1_sq = self.awl.params[1] ** 2   # mipìš©
        theta2_sq = self.awl.params[2] ** 2   # vessel-weightedìš©
        theta3_sq = self.awl.params[3] ** 2   # edge

        # base_loss, mip_lossëŠ” í˜„ì¬ ìŠ¤ì¹¼ë¼ì„(ë°°ì¹˜ í‰ê· ). 
        # baseëŠ” ìŠ¤ì¹¼ë¼ ë¶„ëª¨, MIPëŠ” ìƒ˜í”Œë³„ ë¶„ëª¨ë¥¼ ì‚¬ìš© â†’ per-sample ê°€ì¤‘ ê·¼ì‚¬
        adj_base = theta0_sq                           # scalar
        adj_mip  = theta1_sq + sigma_vec               # [B]  â† ì›ë˜ ë„¤ê°€ per-sampleë¡œ í•˜ë˜ ê±° ìœ ì§€
        adj_vw   = theta2_sq                           # scalar (í•„ìš”í•˜ë©´ ì—¬ê¸°ë„ per-sampleë¡œ ë°”ê¿€ ìˆ˜ ìˆìŒ)

        # ----- ì—¬ê¸°ë¶€í„° ìˆ˜ì •: edge ìª½ì— t-ê¸°ë°˜ sigmoid ìŠ¤ì¼€ì¤„ ë°˜ì˜ -----
        # të¥¼ 0~1ë¡œ ì •ê·œí™”í•´ì„œ ì´ˆê¸°ì—ëŠ” edge ì•½í•˜ê²Œ, í›„ë°˜ì—ëŠ” ê°•í•˜ê²Œ
        T = float(self.num_timesteps)                  # ì˜ˆ: 1000
        t_norm = t.float().view(-1) / (T - 1.0)        # [B], 0~1 ë²”ìœ„

        k      = 8.0   # sigmoid ê¸°ìš¸ê¸° (í´ìˆ˜ë¡ í›„ë°˜ì— ê¸‰ê²©íˆ ì¼œì§)
        center = 0.5   # ì¤‘ê°„ì¯¤ì—ì„œ ì „í™˜
        edge_phase = torch.sigmoid((t_norm - center) * k)   # ì´ˆë°˜â‰ˆ0, í›„ë°˜â‰ˆ1  â†’ [B]

        alpha_edge = 1.0
        # early: edge_phaseâ‰ˆ0 â†’ (1 - edge_phase)â‰ˆ1 â†’ adj_edge ì»¤ì§ â†’ edge term ì•½í•˜ê²Œ
        # late:  edge_phaseâ‰ˆ1 â†’ (1 - edge_phase)â‰ˆ0 â†’ adj_edgeâ‰ˆtheta3_sq â†’ edge term ê°•í•˜ê²Œ
        adj_edge = theta3_sq + alpha_edge * (1.0 - edge_phase)   # [B]
        # ------------------------------------------------------

        total_per = (
            (0.5 / adj_base) * base_loss + torch.log(1 + adj_base)
            + (0.5 / adj_mip)  * mip_loss          + torch.log(1 + adj_mip)
            + (0.5 / adj_vw)   * vessel_weighted_l2 + torch.log(1 + adj_vw)
            + (0.5 / adj_edge) * loss_edge         + torch.log(1 + adj_edge)
        )

        total_loss = total_per.mean()
        # ---------------------------------------------------------------------------2025-10-09 MIP lossë¥¼ ë°°ì¹˜ë§ˆë‹¤ ë‹¤ë¥¸ delta të¡œ ì ìš©í•˜ê¸° mean ì•Ší•˜ê³  Seo ğŸ‘†

        if return_stats:
            stats = {
                'base':        float(base_loss.detach().item()),
                'mip':         float(mip_loss.detach().item()),
                'vessel_l2':   float(vessel_weighted_l2.detach().item()),
                'edge':        float(loss_edge.detach().item()),
                'total':       float(total_loss.detach().item()),
            }
            return total_loss, stats

        return total_loss
        # ---------------------------------------------------------------------------2025-10-09 MIP lossë¥¼ pbarë¡œ ë‚˜íƒ€ë‚´ê¸° Seo ğŸ‘†
        # ===================== ğŸ‘†ğŸ‘†ğŸ‘† ìƒˆ ì˜µì…˜ ì¶”ê°€ ğŸ‘†ğŸ‘†ğŸ‘† =====================


    # -------------------------------------------------20251010 ìˆ˜ì • controlnet update (cond ì „ë‹¬ ê²½ë¡œ ì¶”ê°€)
    def forward(self, img, *args, cond=None, **kwargs):
        b, c, h, w, device, img_size, = *img.shape, img.device, self.image_size
        assert h == img_size[0] and w == img_size[1], f'height and width of image must be {img_size}'
        t = torch.randint(0, self.num_timesteps, (b,), device=device).long()

        img = self.normalize(img)
        return self.p_losses(img, t, *args, cond=cond, **kwargs)





def gumbel_softmax_sample(logits, temperature, gumbel, dim):
    '''mip loss'''
    y = logits + gumbel
    return F.softmax(y / temperature, dim)


def cal_snr(noise_img, clean_img):
    noise_img, clean_img = noise_img.detach().cpu().numpy(), clean_img.detach().cpu().numpy()
    noise_signal = noise_img - clean_img
    clean_signal = clean_img
    noise_signal_2 = noise_signal**2
    clean_signal_2 = clean_signal**2
    sum1 = np.sum(clean_signal_2)
    sum2 = np.sum(noise_signal_2)
    snrr = 20*math.log10(math.sqrt(sum1)/math.sqrt(sum2))
    return snrr


class MIPloss(nn.Module):
    def __init__(self, options):
        super().__init__()
        self.temp = options.mip_loss.temp
        self.num_slice = options.data.use_slice
        # self.L1 = torch.nn.L1Loss()
        self.L2 = torch.nn.MSELoss()

    def reset_gumbel(self, img_fake):
        U = torch.rand_like(img_fake)
        # U = torch.rand(img_fake.size()).cuda()
        self.gumbel = -torch.log(-torch.log(U + 1e-20) + 1e-20)  # sample_gumbel

    def forward(self, img_fake, target):
        self.reset_gumbel(img_fake)
        pred_mips_c1 = torch.zeros_like(img_fake)
        target_mips_c1 = torch.zeros_like(target)
        for idx in range(img_fake.shape[1]):
            pred_mip = gumbel_softmax_sample(img_fake[:, :idx+1], self.temp, self.gumbel[:, :idx+1], dim=1)
            target_mips_c1[:, idx] = torch.max(target[:, :idx+1], dim=1)[0]
            pred_mips_c1[:, idx] = torch.sum(pred_mip*img_fake[:, :idx+1], dim=1)

        pred_mips_c2 = torch.zeros_like(img_fake)
        target_mips_c2 = torch.zeros_like(target)
        for idx in range(img_fake.shape[1]):
            pred_mip = gumbel_softmax_sample(img_fake[:, self.num_slice-idx-1:], self.temp, self.gumbel[:, self.num_slice-idx-1:], dim=1)
            target_mips_c2[:, idx] = torch.max(target[:, self.num_slice-idx-1:], dim=1)[0]
            pred_mips_c2[:, idx] = torch.sum(pred_mip*img_fake[:, self.num_slice-idx-1:], dim=1)
    
        loss_ = self.L2(img_fake, target)
        loss_mip_c1 = self.L2(pred_mips_c1, target_mips_c1)
        loss_mip_c2 = self.L2(pred_mips_c2, target_mips_c2)
        loss = loss_ + loss_mip_c1 + loss_mip_c2

        return loss
        

class AutomaticWeightedLoss(nn.Module):
    def __init__(self, num=4):
        super(AutomaticWeightedLoss, self).__init__()
        params = torch.ones(num, requires_grad=True)
        self.params = torch.nn.Parameter(params)

    def forward(self, losses, sigma_t):
        loss_sum = 0
       
        for i, loss in enumerate(losses):
            if i != 0:
                adjust_para = self.params[i] ** 2 + sigma_t # sigma_t = extract(self.some_sigma_sched, t, x.shape).mean()
            else:
                adjust_para = self.params[i] ** 2
            loss_sum += 0.5 / adjust_para * loss + torch.log(1 + adjust_para)

        return loss_sum





# # dataset classes

# class Dataset(Dataset):
#     def __init__(
#         self,
#         folder,
#         image_size,
#         exts = ['jpg', 'jpeg', 'png', 'tiff', 'npy'],
#         augment_horizontal_flip = False,
#         convert_image_to = None
#     ):
#         super().__init__()
#         self.folder = folder
#         self.image_size = image_size
#         self.paths = [p for ext in exts for p in Path(f'{folder}').glob(f'**/*.{ext}')]

#         maybe_convert_fn = partial(convert_image_to_fn, convert_image_to) if exists(convert_image_to) else nn.Identity()

#         self.transform = T.Compose([
#             T.Lambda(maybe_convert_fn),
#             # T.Resize(image_size), # â˜… Resize ì œê±°, ì„¼í„°í¬ë¡­ë§Œ
#             T.RandomHorizontalFlip() if augment_horizontal_flip else nn.Identity(),
#             T.CenterCrop(image_size),
#             T.ToTensor()
#         ])

#         # âœ… npy ì „ìš© transformë„ Composeë¡œ êµ¬ì„±
#         self.npy_transform = T.Compose([                                          ## SeoSY 2025-07-24 (ê¸°ì¡´ì—ëŠ” PILì— ëŒ€í•´ì„œ transformì´ ì‘ì„±ë˜ì–´ìˆì–´ì„œ ì¶”í›„ì— npyë„ ì—…ë°ì´íŠ¸ ê°€ëŠ¥í•˜ë„ë¡ ë§Œë“¤ì–´ë’€ìŒ)
#             T.Lambda(lambda x: x)  # identity                                     ## SeoSY 2025-07-24
#         ])                                                                        ## SeoSY 2025-07-24

#     def __len__(self):
#         return len(self.paths)

#     def __getitem__(self, index):
#         path = self.paths[index]
#         if path.suffix.lower() == '.npy':                                          ## SeoSY 2025-07-24 (npy ë°ì´í„°ë„ë¶ˆëŸ¬ ì˜¬ ìˆ˜ ìˆë„ë¡ ì¶”ê°€í•¨.)
#             arr = np.load(path)  # shape: (1, H, W), already [0,1]                 ## SeoSY 2025-07-24
#             img = torch.from_numpy(arr).float()                                    ## SeoSY 2025-07-24
#             return self.npy_transform(img)                                         ## SeoSY 2025-07-24
#         else:
#             img = Image.open(path)
#             return self.transform(img)


# #---------------------------------------------------------2025.10.11 dataloaderê´€ë ¨ ìˆ˜ì •
# from torch.utils.data import Dataset as _TorchDataset

# class _PairedDataset(_TorchDataset):
#     """ë‘ ê°œì˜ H5WindowDatasetì„ ê°™ì€ ì¸ë±ìŠ¤ë¡œ ì½ì–´ (a,b) íŠœí”Œì„ ë°˜í™˜"""
#     def __init__(self, ds_a, ds_b, strict: bool = True):
#         self.ds_a = ds_a
#         self.ds_b = ds_b
#         if strict and (len(ds_a) != len(ds_b)):
#             raise ValueError(f"paired dataset length mismatch: {len(ds_a)} vs {len(ds_b)}")
#         self.n = min(len(ds_a), len(ds_b)) if not strict else len(ds_a)
#     def __len__(self):
#         return self.n
#     def __getitem__(self, idx):
#         a = self.ds_a[idx]
#         b = self.ds_b[idx]
#         if isinstance(a, tuple): a = a[0]
#         if isinstance(b, tuple): b = b[0]
#         return (a, b)
# #---------------------------------------------------------2025.10.11 dataloaderê´€ë ¨ ìˆ˜ì •

# trainer class
class Trainer:
    def __init__(
        self,
        diffusion_model,
        folder,
        *,
        train_batch_size = 16,
        gradient_accumulate_every = 1,
        augment_horizontal_flip = True,
        train_lr = 1e-4,
        train_num_steps = 100000,
        ema_update_every = 10,
        ema_decay = 0.995,
        adam_betas = (0.9, 0.99),
        save_and_sample_every = 1000,
        num_samples = 25,
        results_folder = './results',
        amp = False,
        mixed_precision_type = 'fp16',
        split_batches = True,
        convert_image_to = None,
        calculate_fid = True,
        inception_block_idx = 2048,
        max_grad_norm = 1.,
        num_fid_samples = 50000,
        save_best_and_latest_only = False,
        # ==== ì•„ë˜ 5ê°œê°€ ì¶”ê°€ëœ ì¸ì (HDF5ìš©) ====
        # ==== HDF5ìš© ë°ì´í„°ë¡œë” ì¸ì ====
        dataset_key = 'data',
        neighbors = 0,
        val_ratio = 0.05,
        seed = 42,
        rescale = 'none',
        # â–¼ ì¶”ê°€ (í¬ë¡­ + ê²€ì¦ ë¶„í•  ì œì–´)
        crop_size = None,                   # int or (h, w)
        crop_mode_train = 'random',         # 'random' | 'center'
        crop_mode_val   = 'center',         # 'center' ê¶Œì¥
        val_subject_count = None,           # ì˜ˆ: 10 (ë¹„ìœ¨ ëŒ€ì‹  ê°œìˆ˜)
        val_subject_list  = None,           # ['subject_a', 'subject_b', ...]
        # ===================== ğŸ‘‡ğŸ‘‡ğŸ‘‡ ìƒˆ ì˜µì…˜ ì¶”ê°€ ğŸ‘‡ğŸ‘‡ğŸ‘‡ =====================
        use_val_loss_for_best = False,     # Trueë©´ val lossë¡œ best ì €ì¥ì„ ê°€ëŠ¥í•˜ê²Œ
        val_subset_batches = 50,           # í‰ê°€ì— ì‚¬ìš©í•  ë¯¸ë‹ˆë°°ì¹˜ ìˆ˜(ë¹ ë¥¸ í‰ê°€)
        eval_use_val_split = True,         # H5WindowDataset 'val' split ì‚¬ìš© ì‹œë„
        # ===================== ğŸ‘†ğŸ‘†ğŸ‘† ìƒˆ ì˜µì…˜ ì¶”ê°€ ğŸ‘†ğŸ‘†ğŸ‘† =====================

        # --------- 20251010 ControlNet paired loading: cond H5 ì „ë‹¬ìš© ì¸ì ì¶”ê°€ ---------
        cond_h5_path: str | None = None,          # mGRE H5 ê²½ë¡œ (ControlNetì¼ ë•Œë§Œ ì‚¬ìš©)
        cond_dataset_key: str | None = None,      # mGRE H5 ë‚´ dataset key (ê¸°ë³¸: dataset_key)
        cond_rescale: str | None = None,           # mGRE rescale ëª¨ë“œ (ê¸°ë³¸: rescale)
        # --------------------------------------------------------------------------------
        best_policy: str = "val_loss",   # â† ADD

        #----------ìˆ˜ì • 2025-10-20: test ë¶„í•  ê´€ë ¨ ì˜µì…˜ ì¶”ê°€
        test_ratio: float = 0.10,
        test_subject_count = None,
        test_subject_list = None,
        eval_use_test_split: bool = True,
        #----------ìˆ˜ì • 2025-10-20 ë

        # ### ---------------------------------ìˆ˜ì •: z-slice ë²”ìœ„ ì˜µì…˜ ì¶”ê°€
        z_start: int | None = 30,
        z_end:   int | None = 80,
        # ### ---------------------------------ìˆ˜ì • ë
    ):

        super().__init__()

        # accelerator
        self.accelerator = Accelerator(
            split_batches = split_batches,
            mixed_precision = mixed_precision_type if amp else 'no'
        )

        # ===================== âœ… ë©¤ë²„ ì„ ì´ˆê¸°í™” (AttributeError ë°©ì§€) =====================
        self._is_paired = False  # cond_h5_pathê°€ ì£¼ì–´ì ¸ í˜ì–´ ë¡œë”© ê²½ë¡œë¥¼ íƒˆ ë•Œ True
        self.ds = None
        self.ds_cond = None
        self.val_ds = None
        self.val_ds_cond = None
        self.val_dl = None
        self.best_val_loss = float('inf')
        #----------ìˆ˜ì • 2025-10-20: test ê´€ë ¨ ë©¤ë²„ ì´ˆê¸°í™”
        self.test_ds = None
        self.test_ds_cond = None
        self.test_dl = None
        #----------ìˆ˜ì • 2025-10-20 ë
        # ================================================================================

        # model
        self.model = diffusion_model
        self.channels = diffusion_model.channels
        is_ddim_sampling = diffusion_model.is_ddim_sampling

        # default convert_image_to depending on channels
        if not exists(convert_image_to):
            convert_image_to = {1: 'L', 3: 'RGB', 4: 'RGBA'}.get(self.channels)

        # sampling and training hyperparameters
        assert has_int_squareroot(num_samples), 'number of samples must have an integer square root'
        self.num_samples = num_samples
        self.save_and_sample_every = save_and_sample_every
        self.save_best_and_latest_only = save_best_and_latest_only          # [ADD] (ì´ë¯¸ ìˆë‹¤ë©´ ìƒëµ ê°€ëŠ¥)

        self.batch_size = train_batch_size
        self.gradient_accumulate_every = gradient_accumulate_every
        assert (train_batch_size * gradient_accumulate_every) >= 16, f'your effective batch size (train_batch_size x gradient_accumulate_every) should be at least 16 or above'

        self.train_num_steps = train_num_steps
        self.image_size = diffusion_model.image_size

        self.max_grad_norm = max_grad_norm

        img_size_hw = self.image_size
        if isinstance(img_size_hw, int):
            img_size_hw = (img_size_hw, img_size_hw)

        # ### ---------------------------------ìˆ˜ì •: z_start / z_end ë©¤ë²„ë¡œ ì €ì¥
        self.z_start = z_start
        self.z_end   = z_end
        # ### ---------------------------------ìˆ˜ì • ë

        # --------- ControlNet paired loading (single .h5, tuple output) ---------
        if str(folder).lower().endswith('.h5') and (cond_h5_path is not None):
            self._is_paired = True

            # ê°™ì€ íŒŒì¼ì´ë©´ cond_h5_pathë¥¼ folderë¡œ ì •ê·œí™” (êµ³ì´ ë‹¬ë¼ë„ ë˜ì§€ë§Œ ê°™ìœ¼ë©´ I/O 1íšŒ)
            if str(cond_h5_path) != str(folder):
                # ì„œë¡œ ë‹¤ë¥¸ íŒŒì¼ë„ ì§€ì›ì€ ë˜ì§€ë§Œ, í•œ íŒŒì¼ì—ì„œ í‚¤ë§Œ ë‹¤ë¥´ê²Œ ì“°ë ¤ëŠ” ëª©ì ì´ë©´ í†µì¼
                cond_h5_path = folder

            paired_train_ds = H5WindowDataset_CLDM(
                h5_path          = folder,           # í•œ ë²ˆë§Œ ë„˜ê¹€
                target_key       = 'tof',
                cond_key         = 'mgre',
                image_size       = img_size_hw,      # (H,W) ì²´í¬ìš©
                neighbors        = neighbors,
                split            = 'train',
                val_ratio        = 0.1,
                val_subject_count= val_subject_count,
                val_subject_list = val_subject_list,
                seed             = seed,
                rescale          = rescale,                 # target ìŠ¤ì¼€ì¼
                cond_rescale     = (cond_rescale or rescale),  # cond ìŠ¤ì¼€ì¼
                horizontal_flip  = True,              # ì •í•© ìœ ì§€ì•ˆí•˜ê³  í›ˆë ¨ì‹œì—ëŠ” flip í—ˆìš©
                crop_size        = crop_size,
                crop_mode_train  = 'center',           # ëœë¤ í¬ë¡­ ê¸ˆì§€(ì •í•©)
                crop_mode_val    = crop_mode_val,
                return_meta      = False,              # í•„ìš” ì‹œ True
                #----------ìˆ˜ì • 2025-10-20: test ì¸ì ì „ë‹¬
                test_ratio       = test_ratio,
                test_subject_count = test_subject_count,
                test_subject_list  = test_subject_list,
                #----------ìˆ˜ì • 2025-10-20 ë

                # ### ---------------------------------ìˆ˜ì •: z ìŠ¬ë¼ì´ìŠ¤ ì œí•œ ì „ë‹¬
                z_start          = self.z_start,
                z_end            = self.z_end,
                # ### ---------------------------------ìˆ˜ì • ë
            )

            dl = DataLoader(
                paired_train_ds,
                batch_size = train_batch_size,
                shuffle = True,
                pin_memory = True,
                num_workers = 8, # max(1, cpu_count() // 2),                         # HDF5 ë½ ë°©ì§€: ì›Œì»¤ ì¶•ì†Œ
                persistent_workers = True,
            )
            dl = self.accelerator.prepare(dl)
            self.dl = cycle(dl)
            #---------------------------------------------------------2025.10.11 dataloaderê´€ë ¨ ìˆ˜ì •

        else:
            if str(folder).lower().endswith('.h5'):
                # âœ… HDF5 ë™ì  ìœˆë„ìš° ë°ì´í„°ì…‹
                self.ds = H5WindowDataset(
                    h5_path = folder,
                    dataset_key = dataset_key,
                    image_size = img_size_hw,
                    neighbors = neighbors,
                    split = 'train',
                    # ----- ê²€ì¦ ë¶„í•  -----
                    val_ratio = val_ratio,
                    val_subject_count = val_subject_count,
                    val_subject_list = val_subject_list,
                    seed = seed,
                    # ----- ì •ê·œí™” & ì¦ê°• -----
                    rescale = rescale,
                    horizontal_flip = augment_horizontal_flip,
                    # ----- í¬ë¡­ -----
                    crop_size = crop_size,
                    crop_mode_train = crop_mode_train,
                    crop_mode_val   = crop_mode_val,
                    #----------ìˆ˜ì • 2025-10-20: test ì¸ì ì „ë‹¬
                    test_ratio = test_ratio,
                    test_subject_count = test_subject_count,
                    test_subject_list  = test_subject_list,
                    #----------ìˆ˜ì • 2025-10-20 ë

                    # ### ---------------------------------ìˆ˜ì •: z ìŠ¬ë¼ì´ìŠ¤ ì œí•œ ì „ë‹¬
                    z_start = self.z_start,
                    z_end   = self.z_end,
                    # ### ---------------------------------ìˆ˜ì • ë
                )
            else:
                # âœ… ê¸°ì¡´ í´ë” ì´ë¯¸ì§€ ë°ì´í„°ì…‹
                self.ds = Dataset(
                    folder, self.image_size,
                    augment_horizontal_flip = augment_horizontal_flip,
                    convert_image_to = convert_image_to
                )
            # ğŸ” ì—¬ê¸°ì„œ ë°”ë¡œ ê¸¸ì´ ì¶œë ¥
            try:
                _tqdm.write(f"[DATASET] len(self.ds) = {len(self.ds)}")
            except Exception as e:
                _tqdm.write(f"[DATASET] len(self.ds) í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")

            assert len(self.ds) >= 100, 'you should have at least 100 images in your folder. at least 10k images recommended'

            dl = DataLoader(
                self.ds,
                batch_size = train_batch_size,
                shuffle = True,
                pin_memory = True,
                num_workers = 8, # cpu_count()
            )

            # ğŸ” DataLoader ì •ë³´ë„ ì°¸ê³ ë¡œ ì¶œë ¥
            try:
                _tqdm.write(f"[DATALOADER] batch_size = {train_batch_size}, num_workers = {cpu_count()}")
                _tqdm.write(f"[DATALOADER] dataset length seen by dl = {len(dl.dataset)}")
            except Exception:
                pass

            dl = self.accelerator.prepare(dl)
            self.dl = cycle(dl)

        # ===================== ğŸ‘‡ğŸ‘‡ğŸ‘‡ ìƒˆ ë©¤ë²„ ì´ˆê¸°í™” ğŸ‘‡ğŸ‘‡ğŸ‘‡ =====================
        self.use_val_loss_for_best = use_val_loss_for_best
        self.val_subset_batches    = val_subset_batches
        self.eval_use_val_split    = eval_use_val_split
        #----------ìˆ˜ì • 2025-10-20: test ì‚¬ìš© ì—¬ë¶€ ì €ì¥
        self.eval_use_test_split   = eval_use_test_split
        #----------ìˆ˜ì • 2025-10-20 ë

        self.val_dl = None
        self.best_val_loss = float('inf')
        # ===================== ğŸ‘†ğŸ‘†ğŸ‘† ìƒˆ ë©¤ë²„ ì´ˆê¸°í™” ğŸ‘†ğŸ‘†ğŸ‘† =====================

        # ===================== ğŸ‘‡ğŸ‘‡ğŸ‘‡ best-policy ìƒíƒœê°’ ì´ˆê¸°í™” (ADD) ğŸ‘‡ğŸ‘‡ğŸ‘‡ =====================
        self.best_policy = (best_policy or "val_loss").lower()  # â† ADD
        self.best_scores = {                                     # â† ADD
            "FID": float("inf"),
            "val_loss": float("inf"),
            "base": float("inf"),
            "mip": float("inf"),
            "total": float("inf"),
            "total_simple": float("inf"),
            "combo": float("inf"),
        }
        # ===================== ğŸ‘†ğŸ‘†ğŸ‘† best-policy ìƒíƒœê°’ ì´ˆê¸°í™” (ADD) ğŸ‘†ğŸ‘†ğŸ‘† =====================


        # --------- ControlNet paired loading: ê²€ì¦ë„ í˜ì–´ ê²½ë¡œ êµ¬ì„± ---------
        if self._is_paired and self.eval_use_val_split:
            # ê°™ì€ íŒŒì¼ì—ì„œ í‚¤ë§Œ ë‹¤ë¥´ê²Œ ì“¸ ëª©ì ì´ë©´ cond_h5_pathë¥¼ folderë¡œ í†µì¼
            if str(cond_h5_path) != str(folder):
                cond_h5_path = folder

            try:
                paired_val_ds = H5WindowDataset_CLDM(
                    h5_path            = folder,          # í•œ ë²ˆë§Œ ë„˜ê¹€
                    target_key         = 'tof',
                    cond_key           = 'mgre',
                    image_size         = img_size_hw,
                    neighbors          = neighbors,
                    split              = 'val',
                    val_ratio          = val_ratio,
                    val_subject_count  = val_subject_count,
                    val_subject_list   = val_subject_list,
                    seed               = seed,
                    rescale            = rescale,                     # target ìŠ¤ì¼€ì¼
                    cond_rescale       = (cond_rescale or rescale),   # cond ìŠ¤ì¼€ì¼
                    horizontal_flip    = False,                       # ê²€ì¦ aug ê¸ˆì§€
                    crop_size          = crop_size,
                    crop_mode_train    = 'center',
                    crop_mode_val      = 'center',
                    return_meta        = False,
                    #----------ìˆ˜ì • 2025-10-20: test ì¸ì ì „ë‹¬(ê²€ì¦ì—ì„œë„ ë™ì¼ ë¶„í•  ìœ ì§€ ëª©ì )
                    test_ratio         = test_ratio,
                    test_subject_count = test_subject_count,
                    test_subject_list  = test_subject_list,
                    #----------ìˆ˜ì • 2025-10-20 ë

                    # ### ---------------------------------ìˆ˜ì •: z ìŠ¬ë¼ì´ìŠ¤ ì œí•œ ì „ë‹¬ (val)
                    z_start            = self.z_start,
                    z_end              = self.z_end,
                    # ### ---------------------------------ìˆ˜ì • ë
                )
                _tqdm.write(f"[VAL] len(paired_val_ds) = {len(paired_val_ds)}")

                self.val_dl = DataLoader(
                    paired_val_ds,
                    batch_size = 1,
                    shuffle = False,
                    pin_memory = True,
                    num_workers = 8,  # max(1, cpu_count() // 2),
                    persistent_workers = True,
                )
                self.val_dl = self.accelerator.prepare(self.val_dl)

            # except Exception as e:
            #     _tqdm.write(f"[VAL] building paired val loader failed; fallback to train stream for eval: {e}")
            #     self.val_dl = None
            
            except Exception as e:
                _tqdm.write(f"[VAL] building paired val loader failed: {e}")
                raise   # í‰ê°€ ëˆ„ìˆ˜ ë°©ì§€ë¥¼ ìœ„í•´ ì¦‰ì‹œ ì‹¤íŒ¨            #---------------------------------------------------------2025.10.11 dataloaderê´€ë ¨ ìˆ˜ì •

        # ===================== âœ… ì‹±ê¸€ ì „ìš© val ë¶„ê¸°: ë°©ì–´ì  ì²´í¬ë¡œ ìˆ˜ì • =====================
        if (not self._is_paired) and isinstance(getattr(self, "ds", None), H5WindowDataset) and self.eval_use_val_split:
            try:
                self.val_ds = H5WindowDataset(
                    h5_path = folder,
                    dataset_key = dataset_key,
                    image_size = img_size_hw,
                    neighbors = neighbors,
                    split = 'val',
                    val_ratio = val_ratio,
                    val_subject_count = val_subject_count,
                    val_subject_list = val_subject_list,
                    seed = seed,
                    rescale = rescale,
                    horizontal_flip = False,  # ê²€ì¦ì—ëŠ” ë³´í†µ augmentation ë”
                    crop_size = crop_size,
                    crop_mode_train = crop_mode_train,
                    crop_mode_val   = crop_mode_val,
                    # -------------------------------------------------20251010 ìˆ˜ì • controlnet update: ë©”íƒ€ì™€ í•¨ê»˜ ë°˜í™˜ (subject/z ì¶”ì )
                    return_meta = True,
                    #----------ìˆ˜ì • 2025-10-20: test ì¸ì ì „ë‹¬
                    test_ratio = test_ratio,
                    test_subject_count = test_subject_count,
                    test_subject_list  = test_subject_list,
                    #----------ìˆ˜ì • 2025-10-20 ë

                    # ### ---------------------------------ìˆ˜ì •: z ìŠ¬ë¼ì´ìŠ¤ ì œí•œ ì „ë‹¬ (val)
                    z_start = self.z_start,
                    z_end   = self.z_end,
                    # ### ---------------------------------ìˆ˜ì • ë
                )
                _tqdm.write(f"[VAL] len(self.val_ds) = {len(self.val_ds)}")
                val_dl = DataLoader(
                    self.val_ds,
                    # -------------------------------------------------20251010 ìˆ˜ì • controlnet update: ë³¼ë¥¨ ìˆœì„œ ë³´ì¡´ì„ ìœ„í•´ batch_size=1 ê³ ì •
                    batch_size = 1,
                    # -------------------------------------------------20251010 ìˆ˜ì • controlnet update: ê²€ì¦ì—ì„œëŠ” ì…”í”Œ ê¸ˆì§€
                    shuffle = False,
                    pin_memory = True,
                    num_workers = 8, # max(1, cpu_count() // 2)
                )
                self.val_dl = self.accelerator.prepare(val_dl)
            # except Exception as e:
            #     _tqdm.write(f"[VAL] building val loader failed; fallback to train stream for eval: {e}")
            #     self.val_dl = None
            except Exception as e:
                _tqdm.write(f"[VAL] building val loader failed: {e}")
                raise   # í‰ê°€ ëˆ„ìˆ˜ ë°©ì§€ë¥¼ ìœ„í•´ ì¦‰ì‹œ ì‹¤íŒ¨
        # ===================== ğŸ‘†ğŸ‘†ğŸ‘† ì‹±ê¸€ ì „ìš© val ë¶„ê¸° ë ğŸ‘†ğŸ‘†ğŸ‘† =====================

        #----------ìˆ˜ì • 2025-10-20: TEST ë¶„ê¸° ì¶”ê°€ (paired / single ëª¨ë‘)
        # í˜ì–´ë“œ ControlNet í…ŒìŠ¤íŠ¸ ë¡œë”
        if self._is_paired and self.eval_use_test_split:
            if str(cond_h5_path) != str(folder):
                cond_h5_path = folder
            try:
                paired_test_ds = H5WindowDataset_CLDM(
                    h5_path            = folder,
                    target_key         = 'tof',
                    cond_key           = 'mgre',
                    image_size         = img_size_hw,
                    neighbors          = neighbors,
                    split              = 'test',
                    val_ratio          = val_ratio,
                    val_subject_count  = val_subject_count,
                    val_subject_list   = val_subject_list,
                    seed               = seed,
                    rescale            = rescale,
                    cond_rescale       = (cond_rescale or rescale),
                    horizontal_flip    = False,
                    crop_size          = crop_size,
                    crop_mode_train    = 'center',
                    crop_mode_val      = 'center',
                    return_meta        = False,
                    test_ratio         = test_ratio,
                    test_subject_count = test_subject_count,
                    test_subject_list  = test_subject_list,

                    # ### ---------------------------------ìˆ˜ì •: z ìŠ¬ë¼ì´ìŠ¤ ì œí•œ ì „ë‹¬ (test)
                    z_start            = self.z_start,
                    z_end              = self.z_end,
                    # ### ---------------------------------ìˆ˜ì • ë
                )
                _tqdm.write(f"[TEST] len(paired_test_ds) = {len(paired_test_ds)}")
                self.test_dl = DataLoader(
                    paired_test_ds,
                    batch_size = 1,
                    shuffle = False,
                    pin_memory = True,
                    num_workers = 8,
                    persistent_workers = True,
                )
                self.test_dl = self.accelerator.prepare(self.test_dl)
            except Exception as e:
                _tqdm.write(f"[TEST] building paired test loader failed: {e}")
                raise

        # ì‹±ê¸€ í…ŒìŠ¤íŠ¸ ë¡œë”
        if (not self._is_paired) and isinstance(getattr(self, "ds", None), H5WindowDataset) and self.eval_use_test_split:
            try:
                self.test_ds = H5WindowDataset(
                    h5_path = folder,
                    dataset_key = dataset_key,
                    image_size = img_size_hw,
                    neighbors = neighbors,
                    split = 'test',
                    val_ratio = val_ratio,
                    val_subject_count = val_subject_count,
                    val_subject_list = val_subject_list,
                    seed = seed,
                    rescale = rescale,
                    horizontal_flip = False,
                    crop_size = crop_size,
                    crop_mode_train = crop_mode_train,
                    crop_mode_val   = crop_mode_val,
                    return_meta = True,
                    test_ratio = test_ratio,
                    test_subject_count = test_subject_count,
                    test_subject_list  = test_subject_list,

                    # ### ---------------------------------ìˆ˜ì •: z ìŠ¬ë¼ì´ìŠ¤ ì œí•œ ì „ë‹¬ (test)
                    z_start = self.z_start,
                    z_end   = self.z_end,
                    # ### ---------------------------------ìˆ˜ì • ë
                )
                _tqdm.write(f"[TEST] len(self.test_ds) = {len(self.test_ds)}")
                test_dl = DataLoader(
                    self.test_ds,
                    batch_size = 1,
                    shuffle = False,
                    pin_memory = True,
                    num_workers = 8,
                )
                self.test_dl = self.accelerator.prepare(test_dl)
            except Exception as e:
                _tqdm.write(f"[TEST] building test loader failed: {e}")
                raise
        #----------ìˆ˜ì • 2025-10-20 ë






        # optimizer
        # -------------------------------------------------20251010 ìˆ˜ì • controlnet update (requires_grad=True íŒŒë¼ë¯¸í„°ë§Œ ìµœì í™”)
        trainable_params = [p for p in diffusion_model.parameters() if p.requires_grad]
        self.opt = Adam(trainable_params, lr = train_lr, betas = adam_betas)

        # for logging results in a folder periodically

        if self.accelerator.is_main_process:
            self.ema = EMA(diffusion_model, beta = ema_decay, update_every = ema_update_every)
            self.ema.to(self.device)

        self.results_folder = Path(results_folder)
        self.results_folder.mkdir(exist_ok = True)

        # (ADD) TensorBoard writer
        self.tb = None
        if self.accelerator.is_main_process:
            try:
                self.tb = SummaryWriter(log_dir=str(self.results_folder / "tb"))
            except Exception as e:
                _tqdm.write(f"[TB] SummaryWriter init failed: {e}")


        # step counter state

        self.step = 0

        # prepare model, dataloader, optimizer with accelerator

        self.model, self.opt = self.accelerator.prepare(self.model, self.opt)

        # FID-score computation

        self.calculate_fid = calculate_fid and self.accelerator.is_main_process

        if self.calculate_fid:
            from denoising_diffusion_pytorch.fid_evaluation import FIDEvaluation

            if not is_ddim_sampling:
                self.accelerator.print(
                    "WARNING: Robust FID computation requires a lot of generated samples and can therefore be very time consuming."\
                    "Consider using DDIM sampling to save time."
                )

            self.fid_scorer = FIDEvaluation(
                batch_size=self.batch_size,
                dl=self.dl,
                sampler=self.ema.ema_model,
                channels=self.channels,
                accelerator=self.accelerator,
                stats_dir=results_folder,
                device=self.device,
                num_fid_samples=num_fid_samples,
                inception_block_idx=inception_block_idx
            )

        ## ==================== ğŸ‘‡ğŸ‘‡ğŸ‘‡ ë³€ê²½ ì‹œì‘ ğŸ‘‡ğŸ‘‡ğŸ‘‡ =====================
        if save_best_and_latest_only:
            # FID ë˜ëŠ” val loss ì¤‘ í•˜ë‚˜ëŠ” ì¼œì ¸ ìˆì–´ì•¼ í•¨
            if not (calculate_fid or use_val_loss_for_best):
                raise AssertionError(
                    "`save_best_and_latest_only=True`ì´ë©´ í‰ê°€ ê¸°ì¤€ì´ í•„ìš”í•©ë‹ˆë‹¤. "
                    "FID(calculate_fid=True) ë˜ëŠ” val loss(use_val_loss_for_best=True) ì¤‘ í•˜ë‚˜ë¥¼ ì¼œì„¸ìš”."
                )
            # ì´ˆê¸°ê°’ë“¤ ì¤€ë¹„ (ë‘˜ ë‹¤ ì¼  ê²½ìš° ë‘˜ ë‹¤ ì´ˆê¸°í™”)
            if calculate_fid:
                self.best_fid = 1e10
            if use_val_loss_for_best:
                 self.best_val_loss = float('inf')
        ## ==================== ğŸ‘†ğŸ‘†ğŸ‘† ë³€ê²½ ë ğŸ‘†ğŸ‘†ğŸ‘† =====================


        self.save_best_and_latest_only = save_best_and_latest_only

    @property
    def device(self):
        return self.accelerator.device

    def save(self, milestone):
        if not self.accelerator.is_local_main_process:
            return

        data = {
            'step': self.step,
            'model': self.accelerator.get_state_dict(self.model),
            'opt': self.opt.state_dict(),
            'ema': self.ema.state_dict(),
            'scaler': self.accelerator.scaler.state_dict() if exists(self.accelerator.scaler) else None,
            'version': __version__
        }

        # ## ==================== ğŸ‘‡ğŸ‘‡ğŸ‘‡ ë³€ê²½ ì‹œì‘ ğŸ‘‡ğŸ‘‡ğŸ‘‡ ===================== 2025-10-07 SeoSY
        models_dir = self.results_folder / 'models' # modelsë¼ëŠ” í´ë”ë¥¼ ë§Œë“¤ì–´ì„œ ê¹”ë”í•˜ê²Œ ì •ë¦¬í•˜ë ¤ê³  ì¶”ê°€í•œ ê²ƒì„.
        models_dir.mkdir(parents=True, exist_ok=True)
        torch.save(data, str(self.results_folder / f'models/model-{milestone}.pt')) # modelsì— ë“¤ì–´ê°ˆ ìˆ˜ ìˆë„ë¡œ ê²½ë¡œ ì„¤ì •
        ## ==================== ğŸ‘†ğŸ‘†ğŸ‘† ë³€ê²½ ë ğŸ‘†ğŸ‘†ğŸ‘† =====================

    # ---- (ADD) ì‘ì€ ìœ í‹¸: ì›ìì  í…ìŠ¤íŠ¸ ì“°ê¸° ----
    @staticmethod
    def _atomic_write_text(path: Path, text: str):
        tmp = path.with_suffix(path.suffix + ".tmp")
        with open(tmp, "w", encoding="utf-8") as f:
            f.write(text)
        os.replace(tmp, path)

    # ---- (ADD) best/latest ë©”íƒ€ ê¸°ë¡ ----
    def _record_ckpt_info(self, kind: str, *, step: int, epoch: float | None = None,
                          metric_name: str | None = None, metric_value: float | None = None,
                          extra: dict | None = None):
        """
        kind: 'best' or 'latest'
        results_folder/models/model-{kind}.info.json ì— ì´ˆì†Œí˜• ë©”íƒ€ ê¸°ë¡
        """
        import json, time
        models_dir = self.results_folder / "models"
        models_dir.mkdir(parents=True, exist_ok=True)
        payload = {
            "step": int(step),
            "epoch": (None if epoch is None else float(epoch)),
            "metric_name": metric_name,
            "metric_value": (None if metric_value is None else float(metric_value)),
            "wall_time": time.strftime("%Y-%m-%d %H:%M:%S"),
        }
        if extra:
            payload.update(extra)
        info_path = models_dir / f"model-{kind}.info.json"
        self._atomic_write_text(info_path, json.dumps(payload, ensure_ascii=False))


    # ==================== ğŸ‘‡ğŸ‘‡ğŸ‘‡ ìˆ˜ì •: ìœ ì—°í•œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ğŸ‘‡ğŸ‘‡ğŸ‘‡ ====================
    def load(self, milestone, checkpoint_dir=None, strict=True, load_optimizer=True, load_ema=True):
        accelerator = self.accelerator
        device = accelerator.device

        # ---- (ìˆ˜ì •) ì™¸ë¶€ ë””ë ‰í† ë¦¬ì—ì„œë„ ë¡œë“œ ê°€ëŠ¥ ----
        base_dir = Path(checkpoint_dir) if checkpoint_dir is not None else self.results_folder
        ckpt_path = base_dir / f'models/model-{milestone}.pt'

        data = torch.load(str(ckpt_path), map_location=device, weights_only=True)

        # ---- ëª¨ë¸ ê°€ì¤‘ì¹˜: strict ì¸ìë¡œ ìœ ì—° ë¡œë”© ----
        model = self.accelerator.unwrap_model(self.model)
        missing, unexpected = model.load_state_dict(data['model'], strict=strict)
        if not strict:
            if missing:
                print(f"[load] (non-strict) missing keys: {len(missing)} (ì²« 5ê°œ) -> {missing[:5]}")
            if unexpected:
                print(f"[load] (non-strict) unexpected keys: {len(unexpected)} (ì²« 5ê°œ) -> {unexpected[:5]}")

        # ---- ì˜µí‹°ë§ˆì´ì € ìƒíƒœ: êµ¬ì¡° ë‹¬ë¼ì§€ë©´ ìŠ¤í‚µ ----
        step_from_ckpt = int(data.get('step', 0))
        opt_loaded = False
        if load_optimizer and ('opt' in data) and (data['opt'] is not None):
            try:
                self.opt.load_state_dict(data['opt'])
                opt_loaded = True
            except Exception as e:
                print(f"[load] optimizer state skipped (reason: {e})")

        # ---- EMA ìƒíƒœ: ê°€ëŠ¥í•  ë•Œë§Œ ë¡œë“œ, ì‹¤íŒ¨ ì‹œ ìŠ¤í‚µ ----
        if load_ema and self.accelerator.is_main_process and ('ema' in data) and (data['ema'] is not None):
            try:
                self.ema.load_state_dict(data["ema"])
            except Exception as e:
                print(f"[load] EMA state skipped (reason: {e})")

        # ---- ìŠ¤í… ê²°ì •: ì˜µí‹°ë§ˆì´ì € ëª» ë¶ˆë €ìœ¼ë©´ ì˜ë¯¸ê°€ ë‹¤ë¥´ë‹ˆ 0ìœ¼ë¡œ ë¦¬ì…‹ ê¶Œì¥ ----
        if opt_loaded:
            self.step = step_from_ckpt
        else:
            print("[load] optimizer not loaded â†’ step reset to 0 for safety")
            self.step = 0

        if 'version' in data:
            print(f"loading from version {data['version']}")

        # ---- GradScaler ìƒíƒœë„ ê°€ëŠ¥í•˜ë©´ ë¡œë“œ, ì‹¤íŒ¨ ì‹œ ìŠ¤í‚µ ----
        try:
            if exists(self.accelerator.scaler) and exists(data.get('scaler', None)):
                self.accelerator.scaler.load_state_dict(data['scaler'])
        except Exception as e:
            print(f"[load] scaler state skipped (reason: {e})")
    # ==================== ğŸ‘†ğŸ‘†ğŸ‘† ìˆ˜ì • ë ğŸ‘†ğŸ‘†ğŸ‘† ====================



    # (êµì²´) _quick_eval_loss: float -> dict ë¦¬í„´
    # íŒŒì¼: denoising_diffusion_pytorch_SEO_cldm.py (í˜¹ì€ Trainer ì •ì˜ëœ íŒŒì¼)
    @torch.inference_mode()
    def _quick_eval_loss(self, steps: int | None = None):
        """
        ê²€ì¦ ì„œë¸Œì…‹ì—ì„œ í‰ê·  lossë¥¼ ê³„ì‚°í•´ dictë¡œ ë°˜í™˜í•˜ê³ , í…ì„œë³´ë“œì— ê¸°ë¡í•œë‹¤.
        ë°˜í™˜ dict í‚¤:
        - "val_total"          : í‰ê·  total (ê¸°ì¡´ val_lossì— í•´ë‹¹)
        - "val_base"           : í‰ê·  base(MSE)
        - "val_mip"            : í‰ê·  mip
        - "val_total_simple"   : í‰ê·  (base + mip(ìˆìœ¼ë©´))
        - "val_combo"          : í‰ê·  combo (ëª¨ë¸ì´ ì œê³µí•  ë•Œë§Œ)
        """

        was_training = self.model.training
        self.model.eval()

        device = self.device
        steps = steps or getattr(self, "val_subset_batches", 8)
        ema_model = self.ema.ema_model if hasattr(self, "ema") else self.model

        # ============== helpers ==============
        def _tofloat(x):
            if x is None: return None
            return float(x.item()) if hasattr(x, "item") else float(x)

        def _to_tensor_on_device(x):
            # dict ê°™ì€ ê±´ í…ì„œí™”í•˜ì§€ ì•ŠìŒ
            import torch, numpy as np
            if x is None:
                return None
            if isinstance(x, torch.Tensor):
                return x.to(device)
            if isinstance(x, np.ndarray):
                return torch.from_numpy(x).to(device)
            try:
                return torch.as_tensor(x, device=device)
            except Exception:
                return None

        def _extract_tof_cond(batch):
            # dict: ì´ë¯¸ì§€/ì¡°ê±´ í‚¤ë§Œ ë½‘ì•„ì„œ í…ì„œë¡œ, meta ë“±ì€ ë¬´ì‹œ
            if isinstance(batch, dict):
                tof = None
                # ìš°ì„ ìˆœìœ„: 'tof' -> self.dataset_key -> 'data' -> 'img' -> 'image'
                for k in ("tof", getattr(self, "dataset_key", None), "data", "img", "image"):
                    if k and (k in batch):
                        tof = _to_tensor_on_device(batch[k])
                        if tof is not None:
                            break
                cond = None
                for k in ("mgre", "cond"):
                    if k in batch:
                        cond = _to_tensor_on_device(batch[k])
                        if cond is not None:
                            break
                return tof, cond

            # tuple/list: í…ì„œí™” ê°€ëŠ¥í•œ ì²« ìš”ì†Œ tof, ë‹¤ìŒ ìš”ì†Œ cond
            if isinstance(batch, (tuple, list)):
                tof, cond = None, None
                for elem in batch:
                    t = _to_tensor_on_device(elem)
                    if t is None:
                        continue
                    if tof is None:
                        tof = t
                    elif cond is None:
                        cond = t
                        break
                return tof, cond

            # ê¸°íƒ€: ë‹¨ì¼ í…ì„œ/ë°°ì—´ë¡œ ê°„ì£¼
            return _to_tensor_on_device(batch), None
        # =====================================

        # val_dl ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ train streamì—ì„œ ì¼ë¶€ ìŠ¤í… ìƒ˜í”Œ
        iterator = iter(self.val_dl) if getattr(self, "val_dl", None) is not None else None

        sums = dict(total=0.0, base=0.0, mip=0.0, combo=0.0)
        cnts = dict(total=0,   base=0,   mip=0,   combo=0)

        for _ in range(max(1, steps)):
            try:
                batch = next(iterator) if iterator is not None else next(self.dl)
            except StopIteration:
                break

            tof, cond = _extract_tof_cond(batch)
            if tof is None:
                # ì´ë¯¸ì§€ë¥¼ ëª» ì°¾ìœ¼ë©´ í•´ë‹¹ ìŠ¤í…ì€ ìŠ¤í‚µ
                continue

            with self.accelerator.autocast():
                try:
                    out = ema_model(tof, cond=cond, return_stats=True)
                    if isinstance(out, (tuple, list)) and len(out) == 2:
                        loss, stats = out
                    else:
                        # (loss, stats) í˜•íƒœê°€ ì•„ë‹ ìˆ˜ ìˆìŒ
                        loss, stats = out, None
                except TypeError:
                    loss, stats = ema_model(tof, cond=cond), None

            # êµ¬ì„± ìš”ì†Œ ì¶”ì¶œ
            tot   = _tofloat(stats.get("total", loss)) if isinstance(stats, dict) else _tofloat(loss)
            base  = _tofloat(stats.get("base"))  if isinstance(stats, dict) else None
            mip   = _tofloat(stats.get("mip"))   if isinstance(stats, dict) else None
            combo = _tofloat(stats.get("combo")) if isinstance(stats, dict) else None

            if tot  is not None: sums["total"] += tot;  cnts["total"] += 1
            if base is not None: sums["base"]  += base; cnts["base"]  += 1
            if mip  is not None: sums["mip"]   += mip;  cnts["mip"]   += 1
            if combo is not None:sums["combo"] += combo;cnts["combo"] += 1

        def _avg(name):
            return (sums[name] / max(1, cnts[name])) if cnts[name] > 0 else None

        avg_total = _avg("total")
        avg_base  = _avg("base")
        avg_mip   = _avg("mip")
        avg_combo = _avg("combo")
        avg_total_simple = (avg_base + (avg_mip or 0.0)) if (avg_base is not None) else None

        # í…ì„œë³´ë“œ ë¡œê¹… (ì—¬ê¸°ì„œë§Œ ê¸°ë¡í•˜ë©´ ì¤‘ë³µ ì—†ìŒ)
        if getattr(self, "tb", None) is not None:
            if avg_total is not None:
                self.tb.add_scalar("val/loss",  float(avg_total),         global_step=self.step)  # í˜¸í™˜ alias
                self.tb.add_scalar("val/total", float(avg_total),         global_step=self.step)
            if avg_base is not None:
                self.tb.add_scalar("val/base",  float(avg_base),          global_step=self.step)
            if avg_mip is not None:
                self.tb.add_scalar("val/mip",   float(avg_mip),           global_step=self.step)
            if avg_total_simple is not None:
                self.tb.add_scalar("val/total_simple", float(avg_total_simple), global_step=self.step)
            if avg_combo is not None:
                self.tb.add_scalar("val/combo", float(avg_combo),         global_step=self.step)

        val_stats = {
            "val_total":         avg_total,
            "val_base":          avg_base,
            "val_mip":           avg_mip,
            "val_total_simple":  avg_total_simple,
            "val_combo":         avg_combo,
        }
        # ë‹¤ë¥¸ ê³³ì—ì„œ ì°¸ì¡° ê°€ëŠ¥í•˜ë„ë¡ ì €ì¥
        self._last_val_stats = val_stats

        if was_training:
            self.model.train()

        return val_stats

    def _maybe_update_bests(self, fid_score, eval_loss, val_stats):
        """
        best_policyì— ë”°ë¼ best_* ì²´í¬í¬ì¸íŠ¸ë“¤ì„ ê°±ì‹ í•˜ê³ , info.json + TBë¥¼ ê¸°ë¡.
        """
        # latestëŠ” í•­ìƒ
        self.save("latest")
        try:
            self._record_ckpt_info("latest", step=self.step, epoch=getattr(self,"epoch",None),
                                metric_name=None, metric_value=None)
        except Exception as e:
            self.accelerator.print(f"[meta] latest info write skipped: {e}")

        # í›„ë³´ ì§€í‘œ êµ¬ì„±
        candidates = {}
        if fid_score is not None:
            candidates["FID"] = float(fid_score)
        if eval_loss is not None:
            candidates["val_loss"] = float(eval_loss)
        if isinstance(val_stats, dict):
            for src, dst in [
                ("val_base", "base"),
                ("val_mip", "mip"),
                ("val_total", "total"),
                ("val_total_simple", "total_simple"),
                ("val_combo", "combo"),
            ]:
                v = val_stats.get(src, None)
                if v is not None:
                    candidates[dst] = float(v)

        # ì •ì±… í‚¤ ì„ íƒ
        pol = (self.best_policy or "val_loss").lower()
        if pol == "mip":
            keys = ["mip"]
        elif pol == "base":
            keys = ["base"]
        elif pol == "total_simple":
            keys = ["total_simple"]
        elif pol == "val_loss":
            keys = ["val_loss"]
        elif pol == "any":
            keys = [k for k in ("base", "mip", "total_simple") if k in candidates]
            for extra in ("total", "combo"):
                if extra in candidates:
                    keys.append(extra)
        else:
            keys = ["val_loss"]

        improved = {}

        # FID ë³„ë„
        if "FID" in candidates and candidates["FID"] < self.best_scores["FID"]:
            self.best_scores["FID"] = candidates["FID"]
            self.save("best_FID")
            improved["FID"] = candidates["FID"]
            try:
                self._record_ckpt_info("best_FID", step=self.step, epoch=getattr(self,"epoch",None),
                                    metric_name="FID", metric_value=candidates["FID"],
                                    extra=(val_stats or {}))
            except Exception as e:
                self.accelerator.print(f"[meta] best_FID info write skipped: {e}")
            if getattr(self, "tb", None) is not None:
                self.tb.add_scalar("best/FID", float(candidates["FID"]), global_step=self.step)

        # ì„ íƒëœ ì§€í‘œë³„ best ì €ì¥
        for k in keys:
            if k not in candidates:
                continue
            val = candidates[k]
            if val < self.best_scores.get(k, float("inf")):
                self.best_scores[k] = val
                milestone = f"best_{k}"
                self.save(milestone)
                improved[k] = val
                try:
                    self._record_ckpt_info(milestone, step=self.step, epoch=getattr(self,"epoch",None),
                                        metric_name=k, metric_value=val, extra=(val_stats or {}))
                except Exception as e:
                    self.accelerator.print(f"[meta] {milestone} info write skipped: {e}")
                if getattr(self, "tb", None) is not None:
                    self.tb.add_scalar(f"best/{k}", float(val), global_step=self.step)

        if improved and getattr(self, "tb", None) is not None:
            parts = [f"{kk}={improved[kk]:.6f}" for kk in sorted(improved.keys())]
            self.tb.add_text("best/info", f"best at step {self.step} â€” " + ", ".join(parts),
                            global_step=self.step)


    # ===================== visualization helpers (drop-in) =====================

    @staticmethod
    def _to01_from_input(x: torch.Tensor, mode: str = "minus1to1") -> torch.Tensor:
        """
        ì…ë ¥ ë°°ì¹˜ í…ì„œë¥¼ [0,1] ë²”ìœ„ë¡œ ë³€í™˜.
        - mode == 'minus1to1': [-1,1] -> [0,1]
        - mode == 'zeroto1'  : [0,1]  -> [0,1] (ê·¸ëŒ€ë¡œ)
        """
        if mode == "zeroto1":
            y = x
        else:
            # ê¸°ë³¸: [-1,1] ê°€ì •
            y = (x + 1) * 0.5
        return y.clamp(0, 1)   # <-- out-of-place

    @staticmethod
    def _to01_from_model(x: torch.Tensor) -> torch.Tensor:
        """
        ëª¨ë¸ sample()ì´ self.unnormalize()ë¥¼ ê±°ì³ [0,1]ë¡œ ë‚˜ì˜¤ëŠ” ì „ì œ.
        í˜¹ì‹œ ëª¨ë¥¼ ìˆ˜ì¹˜ íŠ€ê¹€ì„ ë°©ì§€í•˜ë ¤ clampë§Œ ìˆ˜í–‰.
        """
        return x.clamp(0, 1)   # <-- out-of-place

    @staticmethod
    def _percentile_stretch(x: torch.Tensor, q_low: float = 0.01, q_high: float = 0.99, eps: float = 1e-6) -> torch.Tensor:
        """
        í¼ì„¼íƒ€ì¼ ê¸°ë°˜ ëŒ€ë¹„ ìŠ¤íŠ¸ë ˆì¹˜.
        ë°°ì¹˜/ì±„ë„ ë³„ë¡œ 2D ê³µê°„(H,W) ì¶•ì— ëŒ€í•´ q_low, q_high í¼ì„¼íƒ€ì¼ì„ êµ¬í•´ ì •ê·œí™”.
        ì…ë ¥ê³¼ ì¶œë ¥ ëª¨ë‘ [0,1] ë²”ìœ„ë¥¼ ê°€ì •/ìœ ì§€.
        """
        assert x.dim() == 4, "expect [B,C,H,W]"
        ql = torch.quantile(x, q_low,  dim=(2, 3), keepdim=True)
        qh = torch.quantile(x, q_high, dim=(2, 3), keepdim=True)
        y = (x - ql) / (qh - ql + eps)
        return y.clamp(0, 1)   # <-- out-of-place

    @staticmethod
    def _maybe_to_rgb(x: torch.Tensor) -> torch.Tensor:
        """
        ë‹¨ì¼ ì±„ë„ì´ë©´ 3ì±„ë„ë¡œ ë³µì œ(ì‹œê°í™” í¸ì˜ìš©).
        ì´ë¯¸ 3ì±„ë„ ì´ìƒì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜.
        """
        if x.dim() != 4:
            raise ValueError("expect [B,C,H,W]")
        if x.size(1) == 1:
            return x.repeat(1, 3, 1, 1)
        return x

    # ===================== end of visualization helpers =====================


    def train(self):
        accelerator = self.accelerator
        device = accelerator.device

        with tqdm(initial = self.step, total = self.train_num_steps, disable = not accelerator.is_main_process) as pbar:

            while self.step < self.train_num_steps:
                self.model.train()

                total_loss = 0.

                #---------------------------------------2025.10.11 ControlNet ë„ì…ë¶€ ìˆ˜ì •
                # í†µê³„ ìˆ˜ì§‘ ì£¼ê¸°: Trainerì— stats_interval ì†ì„±ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ 50
                stats_interval = getattr(self, "stats_interval", 50)
                collect_stats = (self.step % stats_interval == 0)
                # ì´ ìŠ¤í…ì—ì„œ ë¡œê·¸ì— ì“¸ ë§ˆì§€ë§‰ loss í…ì„œ(ë™ê¸°í™”ëŠ” í†µê³„ ìˆ˜ì§‘ì‹œì—ë§Œ)
                step_loss_tensor = None
                #---------------------------------------2025.10.11 ControlNet ë„ì…ë¶€ ìˆ˜ì • (ë)

                for _ in range(self.gradient_accumulate_every):
                    # -------------------------------------------------20251010 ìˆ˜ì • controlnet update (ë°°ì¹˜ íŒŒì‹± + cond ì „ë‹¬, ì•ˆì „í•œ .to)
                    data = next(self.dl)
                    if isinstance(data, (tuple, list)) and len(data) == 2:
                        tof, mgre = data
                    elif isinstance(data, dict) and 'tof' in data and 'mgre' in data:
                        tof, mgre = data['tof'], data['mgre']
                    else:
                        tof, mgre = data, None

                    tof  = tof.to(device)
                    # mgre = mgre.to(device) if mgre is not None else None
                    mgre = (mgre.to(device) if (mgre is not None and torch.is_tensor(mgre)) else None)

                    with self.accelerator.autocast():
                        #---------------------------------------2025.10.11 ControlNet ë„ì…ë¶€ ìˆ˜ì •
                        # NìŠ¤í…ë§ˆë‹¤ë§Œ í†µê³„ ìš”ì²­, ê·¸ ì™¸ì—ëŠ” lossë§Œ ë°˜í™˜ì‹œì¼œ ë™ê¸°í™” ìµœì†Œí™”
                        loss_out = self.model(tof, return_stats=collect_stats, cond=mgre)

                        if collect_stats:
                            # (loss, stats) íŠœí”Œ
                            loss, stats = loss_out
                            last_stats = stats      # ëˆ„ì  í›„ ë¡œê·¸ì— ì‚¬ìš©
                        else:
                            # í†µê³„ ë¯¸ìˆ˜ì§‘ ì‹œ: loss í…ì„œë§Œ ë°›ìŒ
                            loss = loss_out
                            last_stats = None
                        #---------------------------------------2025.10.11 ControlNet ë„ì…ë¶€ ìˆ˜ì • (ë)

                        loss = loss / self.gradient_accumulate_every
                        # ì—¬ê¸°ì„œ .item() í•˜ì§€ ì•ŠìŒ â†’ ë™ê¸°í™” ë°©ì§€
                        step_loss_tensor = loss.detach() if step_loss_tensor is None else (step_loss_tensor + loss.detach())

                    self.accelerator.backward(loss)
                accelerator.wait_for_everyone()
                accelerator.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)

                self.opt.step()
                self.opt.zero_grad()

                accelerator.wait_for_everyone()

                self.step += 1

                # # ì§„í–‰ë°”/ë¡œê·¸ëŠ” main í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ
                # if accelerator.is_main_process:
                #     # 1) ë§¤ ìŠ¤í… ì§„í–‰ë¥  ì¦ê°€
                #     pbar.update(1)

                #     # 2) í†µê³„ ìˆ˜ì§‘ ìŠ¤í…ì—ì„œë§Œ ê°’ ê³„ì‚° + TB ë¡œê¹…
                #     if collect_stats and step_loss_tensor is not None:
                #         def _to_float(x):
                #             if x is None:
                #                 return None
                #             return float(x.item()) if hasattr(x, "item") else float(x)

                #         total_loss = _to_float(step_loss_tensor)
                #         pbar.set_description(f"loss: {total_loss:.4f}")

                #         base_v = mip_v = tot_v = None
                #         if 'last_stats' in locals() and last_stats is not None:
                #             base_v = _to_float(last_stats.get("base"))
                #             mip_v  = _to_float(last_stats.get("mip"))
                #             tot_v  = _to_float(last_stats.get("total"))

                #             if mip_v is None:
                #                 pbar.set_postfix_str(f"base={base_v:.4f} total={tot_v:.4f}")
                #             else:
                #                 pbar.set_postfix_str(f"base={base_v:.4f} mip={mip_v:.4f} total={tot_v:.4f}")

                #         if self.tb is not None:
                #             self.tb.add_scalar("train/loss",  total_loss, global_step=self.step)
                #             if base_v is not None:
                #                 self.tb.add_scalar("train/base",  base_v,  global_step=self.step)
                #             if mip_v is not None:
                #                 self.tb.add_scalar("train/mip",   mip_v,   global_step=self.step)
                #             if tot_v is not None:
                #                 self.tb.add_scalar("train/total", tot_v,   global_step=self.step)

                #     # 3) ì¦‰ì‹œ ì¶œë ¥ ë°˜ì˜(ì˜µì…˜)
                #     pbar.refresh()

                # ì§„í–‰ë°”/ë¡œê·¸ëŠ” main í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ
                if accelerator.is_main_process:
                    # 1) ë§¤ ìŠ¤í… ì§„í–‰ë¥  ì¦ê°€
                    pbar.update(1)

                    # 2) í†µê³„ ìˆ˜ì§‘ ìŠ¤í…ì—ì„œë§Œ ê°’ ê³„ì‚° + TB ë¡œê¹…
                    if collect_stats and step_loss_tensor is not None:
                        def _to_float(x):
                            if x is None:
                                return None
                            return float(x.item()) if hasattr(x, "item") else float(x)

                        total_loss = _to_float(step_loss_tensor)
                        pbar.set_description(f"loss: {total_loss:.4f}")

                        base_v = mip_v = tot_v = None
                        vwl2_v = edge_v = None   # ğŸ‘ˆ ì¶”ê°€

                        if 'last_stats' in locals() and last_stats is not None:
                            base_v   = _to_float(last_stats.get("base"))
                            mip_v    = _to_float(last_stats.get("mip"))
                            vwl2_v   = _to_float(last_stats.get("vessel_l2"))  # ğŸ‘ˆ ì¶”ê°€
                            edge_v   = _to_float(last_stats.get("edge"))       # ğŸ‘ˆ ì¶”ê°€
                            tot_v    = _to_float(last_stats.get("total"))

                            # ì§„í–‰ë°” postfix ë¬¸ìì—´ êµ¬ì„±
                            if mip_v is None:
                                # MIP ë¹„í™œì„±ì¼ ë•Œ
                                pbar.set_postfix_str(
                                    f"base={base_v:.4f} "
                                    f"vwl2={vwl2_v:.4f} "
                                    f"edge={edge_v:.4f} "
                                    f"total={tot_v:.4f}"
                                )
                            else:
                                # MIP í™œì„±ì¼ ë•Œ
                                pbar.set_postfix_str(
                                    f"base={base_v:.4f} "
                                    f"mip={mip_v:.4f} "
                                    f"vwl2={vwl2_v:.4f} "
                                    f"edge={edge_v:.4f} "
                                    f"total={tot_v:.4f}"
                                )

                        if self.tb is not None:
                            self.tb.add_scalar("train/loss",  total_loss, global_step=self.step)
                            if base_v is not None:
                                self.tb.add_scalar("train/base",      base_v,  global_step=self.step)
                            if mip_v is not None:
                                self.tb.add_scalar("train/mip",       mip_v,   global_step=self.step)
                            if vwl2_v is not None:
                                self.tb.add_scalar("train/vessel_l2", vwl2_v,  global_step=self.step)  # ğŸ‘ˆ ì¶”ê°€
                            if edge_v is not None:
                                self.tb.add_scalar("train/edge",      edge_v,  global_step=self.step)  # ğŸ‘ˆ ì¶”ê°€
                            if tot_v is not None:
                                self.tb.add_scalar("train/total",     tot_v,   global_step=self.step)

                    # 3) ì¦‰ì‹œ ì¶œë ¥ ë°˜ì˜(ì˜µì…˜)
                    pbar.refresh()


                # EMAëŠ” ëª¨ë“  í”„ë¡œì„¸ìŠ¤ì—ì„œ ì—…ë°ì´íŠ¸
                if hasattr(self, "ema") and self.ema is not None:
                    self.ema.update()

                    
                    if self.step != 0 and divisible_by(self.step, self.save_and_sample_every):
                        self.ema.ema_model.eval()









                        # ===================== BEGIN PATCH (Seo 2025-09-30) =====================
                        # ---------- ì €ì¥/ì‹œê°í™” ì„¤ì • ----------
                        n = int(math.sqrt(self.num_samples))
                        GRID_COLS = GRID_ROWS = n
                        GRID_COUNT = n * n

                        # vis ì €ì¥ ëª¨ë“œ: "center" | "all" | "tiles" | "center+all"
                        vis_save_mode = "center"

                        # ì…ë ¥ í…ì„œì˜ ë²”ìœ„
                        VIS_INPUT_RANGE = 'minus1to1'

                        APPLY_STRETCH = False
                        Q_LOW, Q_HIGH = 0.01, 0.99
                        replicate_to_rgb = False

                        with torch.inference_mode():
                            milestone = self.step // self.save_and_sample_every

                            # (A) ì‹œê°í™”ìš© ë°°ì¹˜
                            #--------ê³ ì •ê·¸ë¦¬ë“œê·¸ë¦¼ì €ì¥: train ë¡œë” ëŒ€ì‹  val_dlì˜ ì• Nì¥(N=GRID_COUNT)ìœ¼ë¡œ ê³ ì •
                            assert getattr(self, "val_dl", None) is not None, "val_dlì´ ì—†ìŠµë‹ˆë‹¤. eval_use_val_split=Trueì¸ì§€ í™•ì¸í•˜ì„¸ìš”."
                            N = GRID_COUNT
                            xs, cs = [], []
                            val_it = iter(self.val_dl)   # val_dl: batch_size=1, shuffle=False
                            for _ in range(N):
                                batch = next(val_it)
                                if isinstance(batch, (tuple, list)) and len(batch) >= 2:
                                    x, c = batch[0], batch[1]
                                elif isinstance(batch, dict) and 'tof' in batch and 'mgre' in batch:
                                    x, c = batch['tof'], batch['mgre']
                                else:
                                    x, c = batch, None
                                xs.append(x)
                                if c is not None:
                                    cs.append(c)

                            vis_batch = torch.cat([t.to(self.device) for t in xs], dim=0)  # [N,C,H,W]

                            ### 2025-10-29 ì¡°ê±´ë¶€ ëª¨ë¸ìš© ì¡°ê±´ í…ì„œ ë³‘í•© (ì´ê²Œ pretrianì—ì„œ ë™ì‘í• ë•Œ conditionì´ ì—†ì–´ì„œ ìƒê¸°ëŠ” ì˜¤ë¥˜ë¥¼ ì—†ì• ê¸° ìœ„í•œ ì½”ë“œì„)
                            # vis_cond  = (torch.cat([t.to(self.device) for t in cs], dim=0) if (cs and len(cs)==len(xs)) else None)
                            vis_cond = None
                            if getattr(self.model, "use_control", False):
                                # ì—¬ê¸°ì„œë§Œ vis_cond ë§Œë“¤ê¸°
                                if cs and len(cs) == len(xs) and isinstance(cs, (list, tuple)) and all(hasattr(t, "to") for t in cs):
                                    vis_cond = torch.cat([t.to(self.device) for t in cs], dim=0)


                            # print(vis_batch.shape, vis_batch.dtype, vis_batch.min().item(), vis_batch.max().item())
                            # print(vis_cond.shape,  vis_cond.dtype,  vis_cond.min().item(),  vis_cond.max().item()) if vis_cond is not None else None

                            # for iii in range(vis_cond.shape[1]) if vis_cond is not None else []:
                            #     print(vis_cond[:, iii, ...].shape, vis_cond[:, iii, ...].dtype, vis_cond[:, iii, ...].min().item(), vis_cond[:, iii, ...].max().item())

                            # if vis_cond is not None:
                            #     import matplotlib.pyplot as plt
                            #     conditions_dir = self.results_folder / 'conditions'
                            #     conditions_dir.mkdir(parents=True, exist_ok=True)
                            #     plt.figure(figsize=(12,6))
                            #     plt.imshow(vis_cond[0, 2].cpu(), cmap='gray')
                            #     plt.colorbar()
                            #     plt.savefig(self.results_folder / f'conditions/cond-sample-{milestone}.png', dpi=150)
                            #     plt.close()

                            B, C, H, W = vis_batch.shape
                            center_idx = (C - 1) // 2
                            # vis_batch = vis_batch[:min(B, GRID_COUNT)]  # ê³ ì • Nê°œë¥¼ ì´ë¯¸ ë½‘ì•˜ìœ¼ë¯€ë¡œ ìŠ¬ë¼ì´ìŠ¤ ë¶ˆí•„ìš”
                            # if vis_cond is not None:
                            #     vis_cond = vis_cond[:vis_batch.size(0)]

                            #----------------------------2025.10.12 condition png ìƒì„±í•˜ê¸°
                            # (1) ì €ì¥ìš©ìœ¼ë¡œ ì›ë³¸ cond ë°°ì¹˜ë¥¼ ë³´ì¡´
                            vis_cond_for_save = vis_cond

                            # (2) mGREê°€ [B,25,H,W] (= echo 5 * neighbor 5)ë¼ê³  ê°€ì • â†’ ì²« ë²ˆì§¸ ì—ì½”ì˜ 5ìŠ¬ë¼ì´ìŠ¤ë§Œ [B,5,H,W]
                            def _first_echo_group(cond_tensor, neighbor_slices=5):
                                if cond_tensor is None:
                                    return None
                                Bc, Cc, Hc, Wc = cond_tensor.shape
                                if Cc < neighbor_slices:     # ì•ˆì „ì¥ì¹˜
                                    return cond_tensor
                                return cond_tensor[:, 0:neighbor_slices, ...]   # ì±„ë„ 0~4 = 1ë²ˆì§¸ ì—ì½”ì˜ 5ìŠ¬ë¼ì´ìŠ¤
                            cond_5 = _first_echo_group(vis_cond_for_save, neighbor_slices=5)   # [B,5,H,W] or None
                            #----------------------------2025.10.12 condition png ìƒì„±í•˜ê¸° (ë)

                            # (C) ëœë¤ ìƒ˜í”Œë„ ë™ì¼ ê·¸ë¦¬ë“œ í¬ê¸°ë¡œ (ControlNet cond ì „ë‹¬)
                            all_images_list = []
                            remain = GRID_COUNT

                            #----------------------------2025.10.12 condition png ìƒì„±í•˜ê¸°
                            # ìƒ˜í”Œë§ì—ë§Œ ì‚¬ìš©í•  í¬ì¸í„°(ì´ ë³€ìˆ˜ë§Œ ì¬ë°”ì¸ë”©í•˜ì—¬ 'ì†Œëª¨')
                            vis_cond_for_sampling = vis_cond
                            #----------------------------2025.10.12 condition png ìƒì„±í•˜ê¸° (ë)

                            while remain > 0:
                                n = min(self.batch_size, remain)
                                #----------------------------2025.10.12 condition png ìƒì„±í•˜ê¸°
                                cond_slice = (vis_cond_for_sampling[:n] if vis_cond_for_sampling is not None else None)
                                #----------------------------2025.10.12 condition png ìƒì„±í•˜ê¸° (ë)
                                all_images_list.append(self.ema.ema_model.sample(batch_size=n, cond=cond_slice))  # [n,C,H,W], [0,1]
                                #----------------------------2025.10.12 condition png ìƒì„±í•˜ê¸°
                                if vis_cond_for_sampling is not None:
                                    vis_cond_for_sampling = vis_cond_for_sampling[n:]
                                #----------------------------2025.10.12 condition png ìƒì„±í•˜ê¸° (ë)
                                remain -= n
                            all_images = torch.cat(all_images_list, dim=0)  # [GRID_COUNT, C, H, W]
                            
                            print("INPUT stats:", vis_batch.amin().item(), vis_batch.mean().item(), vis_batch.amax().item())
                            print("INPUT shape:", tuple(vis_batch.shape))
                            
                            print("SAMPLE stats:", all_images.amin().item(), all_images.mean().item(), all_images.amax().item())
                            print("SAMPLE shape:", tuple(all_images.shape))

                        # (D) ì €ì¥
                        inputs_dir  = self.results_folder / 'inputs'
                        samples_dir = self.results_folder / 'samples'
                        inputs_dir.mkdir(parents=True, exist_ok=True)
                        samples_dir.mkdir(parents=True, exist_ok=True)

                        #----------------------------2025.10.12 condition png ìƒì„±í•˜ê¸°
                        # condition ì „ìš© í´ë”
                        conditions_dir = self.results_folder / 'conditions'
                        conditions_dir.mkdir(parents=True, exist_ok=True)
                        print(conditions_dir)
                        #----------------------------2025.10.12 condition png ìƒì„±í•˜ê¸° (ë)


                        # (1) ì¤‘ì•™ ì±„ë„ë§Œ ì €ì¥ â€” ëª¨ë‘ n x n ê·¸ë¦¬ë“œ
                        if vis_save_mode in ("center", "center+all"):
                            vis_c = vis_batch[:, center_idx:center_idx+1]
                            sample_c = all_images[:, center_idx:center_idx+1]

                            if APPLY_STRETCH:
                                vis_c    = self._percentile_stretch(vis_c,    Q_LOW, Q_HIGH)
                                sample_c = self._percentile_stretch(sample_c, Q_LOW, Q_HIGH)

                            vis_c    = self._maybe_to_rgb(vis_c)
                            sample_c = self._maybe_to_rgb(sample_c)

                            utils.save_image(vis_c,    str(inputs_dir  / f'input-center-{milestone}.png'),  nrow=GRID_COLS, normalize=True, value_range=(-1,1), scale_each=True)
                            utils.save_image(sample_c, str(samples_dir / f'sample-center-{milestone}.png'), nrow=GRID_COLS, normalize=True, value_range=(-1,1), scale_each=True)

                            #----------------------------2025.10.12 condition png ìƒì„±í•˜ê¸°
                            # cond_5([B,5,H,W])ê°€ ìˆìœ¼ë©´ ì¤‘ì•™ ì±„ë„(5â†’idx=2)ì„ input/sampleê³¼ ë™ì¼ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì €ì¥
                            if cond_5 is not None and cond_5.shape[1] > 0:
                                cond_center_idx = (cond_5.shape[1] - 1) // 2    # 5ì±„ë„ì´ë©´ 2
                                cond_c = cond_5[:, cond_center_idx:cond_center_idx+1]   # [B,1,H,W]
                                if APPLY_STRETCH:
                                    cond_c = self._percentile_stretch(cond_c, Q_LOW, Q_HIGH)
                                cond_c = self._maybe_to_rgb(cond_c)
                                utils.save_image(
                                    cond_c,
                                    str(conditions_dir / f'condition-center-{milestone}.png'),
                                    nrow=GRID_COLS, normalize=True, value_range=(-1,1), scale_each=True
                                )
                            #----------------------------2025.10.12 condition png ìƒì„±í•˜ê¸° (ë)



                        # # ===================== BEGIN PATCH (Seo 2025-09-30) =====================
                        # # ---------- ì €ì¥/ì‹œê°í™” ì„¤ì • ----------
                        # n = int(math.sqrt(self.num_samples))
                        # GRID_COLS = GRID_ROWS = n
                        # GRID_COUNT = n * n

                        # # vis ì €ì¥ ëª¨ë“œ: "center" | "all" | "tiles" | "center+all"
                        # vis_save_mode = "center"

                        # # ì…ë ¥ í…ì„œì˜ ë²”ìœ„
                        # VIS_INPUT_RANGE = 'minus1to1'

                        # APPLY_STRETCH = False
                        # Q_LOW, Q_HIGH = 0.01, 0.99
                        # replicate_to_rgb = False

                        # with torch.inference_mode():
                        #     milestone = self.step // self.save_and_sample_every

                        #     # (A) ì‹œê°í™”ìš© ë°°ì¹˜
                        #     vis_raw = next(self.dl)
                        #     if isinstance(vis_raw, (tuple, list)) and len(vis_raw) == 2:
                        #         vis_batch, vis_cond = vis_raw
                        #     elif isinstance(vis_raw, dict) and 'tof' in vis_raw and 'mgre' in vis_raw:
                        #         vis_batch, vis_cond = vis_raw['tof'], vis_raw['mgre']
                        #     else:
                        #         vis_batch, vis_cond = vis_raw, None

                        #     vis_batch = vis_batch.to(self.device)
                        #     vis_cond  = vis_cond.to(self.device) if vis_cond is not None else None

                        #     # print(vis_batch.shape, vis_batch.dtype, vis_batch.min().item(), vis_batch.max().item())
                        #     # print(vis_cond.shape,  vis_cond.dtype,  vis_cond.min().item(),  vis_cond.max().item()) if vis_cond is not None else None

                        #     # for iii in range(vis_cond.shape[1]) if vis_cond is not None else []:
                        #     #     print(vis_cond[:, iii, ...].shape, vis_cond[:, iii, ...].dtype, vis_cond[:, iii, ...].min().item(), vis_cond[:, iii, ...].max().item())

                        #     # if vis_cond is not None:
                        #     #     import matplotlib.pyplot as plt
                        #     #     conditions_dir = self.results_folder / 'conditions'
                        #     #     conditions_dir.mkdir(parents=True, exist_ok=True)
                        #     #     plt.figure(figsize=(12,6))
                        #     #     plt.imshow(vis_cond[0, 2].cpu(), cmap='gray')
                        #     #     plt.colorbar()
                        #     #     plt.savefig(self.results_folder / f'conditions/cond-sample-{milestone}.png', dpi=150)
                        #     #     plt.close()

                        #     B, C, H, W = vis_batch.shape
                        #     center_idx = (C - 1) // 2
                        #     vis_batch = vis_batch[:min(B, GRID_COUNT)]
                        #     if vis_cond is not None:
                        #         vis_cond = vis_cond[:vis_batch.size(0)]

                        #     #----------------------------2025.10.12 condition png ìƒì„±í•˜ê¸°
                        #     # (1) ì €ì¥ìš©ìœ¼ë¡œ ì›ë³¸ cond ë°°ì¹˜ë¥¼ ë³´ì¡´
                        #     vis_cond_for_save = vis_cond

                        #     # (2) mGREê°€ [B,25,H,W] (= echo 5 * neighbor 5)ë¼ê³  ê°€ì • â†’ ì²« ë²ˆì§¸ ì—ì½”ì˜ 5ìŠ¬ë¼ì´ìŠ¤ë§Œ [B,5,H,W]
                        #     def _first_echo_group(cond_tensor, neighbor_slices=5):
                        #         if cond_tensor is None:
                        #             return None
                        #         Bc, Cc, Hc, Wc = cond_tensor.shape
                        #         if Cc < neighbor_slices:     # ì•ˆì „ì¥ì¹˜
                        #             return cond_tensor
                        #         return cond_tensor[:, 0:neighbor_slices, ...]   # ì±„ë„ 0~4 = 1ë²ˆì§¸ ì—ì½”ì˜ 5ìŠ¬ë¼ì´ìŠ¤
                        #     cond_5 = _first_echo_group(vis_cond_for_save, neighbor_slices=5)   # [B,5,H,W] or None
                        #     #----------------------------2025.10.12 condition png ìƒì„±í•˜ê¸° (ë)

                        #     # (C) ëœë¤ ìƒ˜í”Œë„ ë™ì¼ ê·¸ë¦¬ë“œ í¬ê¸°ë¡œ (ControlNet cond ì „ë‹¬)
                        #     all_images_list = []
                        #     remain = GRID_COUNT

                        #     #----------------------------2025.10.12 condition png ìƒì„±í•˜ê¸°
                        #     # ìƒ˜í”Œë§ì—ë§Œ ì‚¬ìš©í•  í¬ì¸í„°(ì´ ë³€ìˆ˜ë§Œ ì¬ë°”ì¸ë”©í•˜ì—¬ 'ì†Œëª¨')
                        #     vis_cond_for_sampling = vis_cond
                        #     #----------------------------2025.10.12 condition png ìƒì„±í•˜ê¸° (ë)

                        #     while remain > 0:
                        #         n = min(self.batch_size, remain)
                        #         #----------------------------2025.10.12 condition png ìƒì„±í•˜ê¸°
                        #         cond_slice = (vis_cond_for_sampling[:n] if vis_cond_for_sampling is not None else None)
                        #         #----------------------------2025.10.12 condition png ìƒì„±í•˜ê¸° (ë)
                        #         all_images_list.append(self.ema.ema_model.sample(batch_size=n, cond=cond_slice))  # [n,C,H,W], [0,1]
                        #         #----------------------------2025.10.12 condition png ìƒì„±í•˜ê¸°
                        #         if vis_cond_for_sampling is not None:
                        #             vis_cond_for_sampling = vis_cond_for_sampling[n:]
                        #         #----------------------------2025.10.12 condition png ìƒì„±í•˜ê¸° (ë)
                        #         remain -= n
                        #     all_images = torch.cat(all_images_list, dim=0)  # [GRID_COUNT, C, H, W]
                            
                        #     print("INPUT stats:", vis_batch.amin().item(), vis_batch.mean().item(), vis_batch.amax().item())
                        #     print("INPUT shape:", tuple(vis_batch.shape))
                            
                        #     print("SAMPLE stats:", all_images.amin().item(), all_images.mean().item(), all_images.amax().item())
                        #     print("SAMPLE shape:", tuple(all_images.shape))

                        # # (D) ì €ì¥
                        # inputs_dir  = self.results_folder / 'inputs'
                        # samples_dir = self.results_folder / 'samples'
                        # inputs_dir.mkdir(parents=True, exist_ok=True)
                        # samples_dir.mkdir(parents=True, exist_ok=True)

                        # #----------------------------2025.10.12 condition png ìƒì„±í•˜ê¸°
                        # # condition ì „ìš© í´ë”
                        # conditions_dir = self.results_folder / 'conditions'
                        # conditions_dir.mkdir(parents=True, exist_ok=True)
                        # print(conditions_dir)
                        # #----------------------------2025.10.12 condition png ìƒì„±í•˜ê¸° (ë)


                        # # (1) ì¤‘ì•™ ì±„ë„ë§Œ ì €ì¥ â€” ëª¨ë‘ n x n ê·¸ë¦¬ë“œ
                        # if vis_save_mode in ("center", "center+all"):
                        #     vis_c = vis_batch[:, center_idx:center_idx+1]
                        #     sample_c = all_images[:, center_idx:center_idx+1]

                        #     if APPLY_STRETCH:
                        #         vis_c    = self._percentile_stretch(vis_c,    Q_LOW, Q_HIGH)
                        #         sample_c = self._percentile_stretch(sample_c, Q_LOW, Q_HIGH)

                        #     vis_c    = self._maybe_to_rgb(vis_c)
                        #     sample_c = self._maybe_to_rgb(sample_c)

                        #     utils.save_image(vis_c,    str(inputs_dir  / f'input-center-{milestone}.png'),  nrow=GRID_COLS, normalize=True, value_range=(-1,1), scale_each=True)
                        #     utils.save_image(sample_c, str(samples_dir / f'sample-center-{milestone}.png'), nrow=GRID_COLS, normalize=True, value_range=(-1,1), scale_each=True)

                        #     #----------------------------2025.10.12 condition png ìƒì„±í•˜ê¸°
                        #     # cond_5([B,5,H,W])ê°€ ìˆìœ¼ë©´ ì¤‘ì•™ ì±„ë„(5â†’idx=2)ì„ input/sampleê³¼ ë™ì¼ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì €ì¥
                        #     if cond_5 is not None and cond_5.shape[1] > 0:
                        #         cond_center_idx = (cond_5.shape[1] - 1) // 2    # 5ì±„ë„ì´ë©´ 2
                        #         cond_c = cond_5[:, cond_center_idx:cond_center_idx+1]   # [B,1,H,W]
                        #         if APPLY_STRETCH:
                        #             cond_c = self._percentile_stretch(cond_c, Q_LOW, Q_HIGH)
                        #         cond_c = self._maybe_to_rgb(cond_c)
                        #         utils.save_image(
                        #             cond_c,
                        #             str(conditions_dir / f'condition-center-{milestone}.png'),
                        #             nrow=GRID_COLS, normalize=True, value_range=(-1,1), scale_each=True
                        #         )
                        #     #----------------------------2025.10.12 condition png ìƒì„±í•˜ê¸° (ë)
                            # ===================== END PATCH (Seo 2025-09-30) =====================








                            # ===================== END PATCH (Seo 2025-09-30) =====================

                            # whether to calculate fid
                            if self.calculate_fid:
                                fid_score = self.fid_scorer.fid_score()
                                accelerator.print(f'fid_score: {fid_score}')
                            else:
                                fid_score = None  # ğŸ‘ˆ FID ë¹„í™œì„±í™” ì‹œ None

                            # ===================== ğŸ‘‡ğŸ‘‡ğŸ‘‡ ìƒˆë¡œ ì¶”ê°€: val loss í‰ê°€ (êµ¬ì„±ìš”ì†Œë³„) ğŸ‘‡ğŸ‘‡ğŸ‘‡ =====================
                            val_stats = None  # {"val_total","val_base","val_mip","val_total_simple","val_combo"}
                            if self.use_val_loss_for_best:
                                try:
                                    # _quick_eval_loss ê°€ dict ë˜ëŠ” (avg, dict) ë°˜í™˜í•˜ëŠ” ê²½ìš° ëª¨ë‘ ì§€ì›
                                    _ret = self._quick_eval_loss()
                                    if isinstance(_ret, tuple):
                                        _, val_stats = _ret
                                    else:
                                        val_stats = _ret

                                    # total_simpleì´ ì—†ìœ¼ë©´ base(+mip)ë¡œ ê³„ì‚°
                                    if val_stats is not None and val_stats.get("val_total_simple") is None:
                                        vb = val_stats.get("val_base", None)
                                        vm = val_stats.get("val_mip",  0.0)
                                        if vb is not None:
                                            val_stats["val_total_simple"] = float(vb) + (0.0 if vm is None else float(vm))

                                    vt = val_stats.get("val_total", None) if val_stats else None
                                    vb = val_stats.get("val_base",  None) if val_stats else None
                                    vm = val_stats.get("val_mip",   None) if val_stats else None
                                    vs = val_stats.get("val_total_simple", None) if val_stats else None
                                    msg = [f"total={vt:.6f}" if vt is not None else "total=None"]
                                    if vb is not None: msg.append(f"base={vb:.6f}")
                                    if vm is not None: msg.append(f"mip={vm:.6f}")
                                    if vs is not None: msg.append(f"total_simple={vs:.6f}")
                                    accelerator.print(f'val (subset~{self.val_subset_batches} batches): ' + " ".join(msg))
                                except Exception as e:
                                    accelerator.print(f'val eval skipped due to error: {e}')
                                    val_stats = None
                            # ===================== ğŸ‘†ğŸ‘†ğŸ‘† ìƒˆë¡œ ì¶”ê°€: val loss í‰ê°€ (êµ¬ì„±ìš”ì†Œë³„) ğŸ‘†ğŸ‘†ğŸ‘† =====================

                            # ===================== ğŸ‘‡ğŸ‘‡ğŸ‘‡ (ADD) TB: ê²€ì¦ ìŠ¤ì¹¼ë¼ ë¡œê¹… ğŸ‘‡ğŸ‘‡ğŸ‘‡ =====================
                            if getattr(self, "tb", None) is not None:
                                if fid_score is not None:
                                    self.tb.add_scalar("val/FID", float(fid_score), global_step=self.step)
                                if val_stats is not None:
                                    vt = val_stats.get("val_total", None)
                                    vb = val_stats.get("val_base",  None)
                                    vm = val_stats.get("val_mip",   None)
                                    vs = val_stats.get("val_total_simple", None)
                                    if vt is not None:
                                        # í˜¸í™˜ì„±: ê¸°ì¡´ "val/loss"ëŠ” totalì„ ê·¸ëŒ€ë¡œ ê¸°ë¡
                                        self.tb.add_scalar("val/loss",  float(vt), global_step=self.step)
                                        self.tb.add_scalar("val/total", float(vt), global_step=self.step)
                                    if vb is not None:
                                        self.tb.add_scalar("val/base",  float(vb), global_step=self.step)
                                    if vm is not None:
                                        self.tb.add_scalar("val/mip",   float(vm), global_step=self.step)
                                    if vs is not None:
                                        self.tb.add_scalar("val/total_simple", float(vs), global_step=self.step)
                            # ===================== ğŸ‘†ğŸ‘†ğŸ‘† (ADD) TB: ê²€ì¦ ìŠ¤ì¹¼ë¼ ë¡œê¹… ğŸ‘†ğŸ‘†ğŸ‘† =====================

                            if self.save_best_and_latest_only:
                                # â–¼ ì•ˆì „ì¥ì¹˜: policy/score í…Œì´ë¸”ì´ ì—†ë‹¤ë©´ ì—¬ê¸°ì„œ 1íšŒ ì´ˆê¸°í™”
                                if not hasattr(self, "best_policy"):
                                    self.best_policy = "val_loss"
                                if not hasattr(self, "best_scores"):
                                    self.best_scores = {
                                        "FID": float("inf"),
                                        "val_loss": float("inf"),
                                        "base": float("inf"),
                                        "mip": float("inf"),
                                        "total": float("inf"),
                                        "total_simple": float("inf"),
                                        "combo": float("inf"),
                                    }

                                did_save_best = False
                                _improved = {}   # (ADD) ì–´ë–¤ ì§€í‘œê°€ ê°œì„ ë˜ì—ˆëŠ”ì§€ ê¸°ë¡

                                # latestëŠ” í•­ìƒ ë¨¼ì € ì €ì¥ + ë©”íƒ€ ê¸°ë¡ ì¤€ë¹„
                                # JSON(info)ì— ë„£ì„ extra payload (train/val êµ¬ì„±ìš”ì†Œ) êµ¬ì„±
                                def _to_float(x):
                                    if x is None: return None
                                    return float(x.item()) if hasattr(x, "item") else float(x)

                                train_extra = {}
                                if 'last_stats' in locals() and last_stats is not None:
                                    train_extra = {
                                        "train_base":  _to_float(last_stats.get("base")),
                                        "train_mip":   _to_float(last_stats.get("mip")),
                                        "train_total": _to_float(last_stats.get("total")),
                                    }

                                val_extra = {}
                                if val_stats is not None:
                                    val_extra = {
                                        "val_base":         None if val_stats.get("val_base")         is None else float(val_stats["val_base"]),
                                        "val_mip":          None if val_stats.get("val_mip")          is None else float(val_stats["val_mip"]),
                                        "val_total":        None if val_stats.get("val_total")        is None else float(val_stats["val_total"]),
                                        "val_total_simple": None if val_stats.get("val_total_simple") is None else float(val_stats["val_total_simple"]),
                                        "val_combo":        None if val_stats.get("val_combo")        is None else float(val_stats["val_combo"]),
                                    }

                                extra_payload = {}
                                extra_payload.update(train_extra)
                                extra_payload.update(val_extra)

                                self.save("latest")
                                try:
                                    self._record_ckpt_info(
                                        "latest",
                                        step=self.step,
                                        epoch=getattr(self, "epoch", None),
                                        metric_name=None,
                                        metric_value=None,
                                        extra=extra_payload,   # â˜… ì¶”ê°€
                                    )
                                except Exception as e:
                                    accelerator.print(f"[meta] latest info write skipped: {e}")

                                # ---------- í›„ë³´ ì§€í‘œ êµ¬ì„± ----------
                                candidates = {}
                                if fid_score is not None:
                                    candidates["FID"] = float(fid_score)
                                # val_loss(=total)ì„ ê¸°ë³¸ í›„ë³´ë¡œ
                                vt = val_stats.get("val_total", None) if val_stats is not None else None
                                if vt is not None:
                                    candidates["val_loss"] = float(vt)
                                    candidates["total"]    = float(vt)
                                # êµ¬ì„±ìš”ì†Œ í›„ë³´ë“¤
                                if val_stats is not None:
                                    if val_stats.get("val_base") is not None:
                                        candidates["base"] = float(val_stats["val_base"])
                                    if val_stats.get("val_mip") is not None:
                                        candidates["mip"] = float(val_stats["val_mip"])
                                    if val_stats.get("val_total_simple") is not None:
                                        candidates["total_simple"] = float(val_stats["val_total_simple"])
                                    if val_stats.get("val_combo") is not None:
                                        candidates["combo"] = float(val_stats["val_combo"])

                                # ---------- ì •ì±… ê²°ì • ----------
                                pol = (self.best_policy or "val_loss").lower()
                                if pol == "mip":
                                    keys = ["mip"]
                                elif pol == "base":
                                    keys = ["base"]
                                elif pol == "total_simple":
                                    keys = ["total_simple"]
                                elif pol == "val_loss":
                                    keys = ["val_loss"]
                                elif pol == "any":
                                    keys = [k for k in ("base", "mip", "total_simple") if k in candidates]
                                    for extra_k in ("total", "combo"):
                                        if extra_k in candidates:
                                            keys.append(extra_k)
                                else:
                                    keys = ["val_loss"]

                                # ---------- FID ê°œì„  ì²˜ë¦¬ (ë³„ë„ best_FID.*) ----------
                                if "FID" in candidates and candidates["FID"] < self.best_scores["FID"]:
                                    self.best_scores["FID"] = candidates["FID"]
                                    self.save("best_FID")
                                    did_save_best = True
                                    _improved["FID"] = float(candidates["FID"])
                                    try:
                                        self._record_ckpt_info(
                                            "best_FID",
                                            step=self.step,
                                            epoch=getattr(self, "epoch", None),
                                            metric_name="FID",
                                            metric_value=float(candidates["FID"]),
                                            extra=extra_payload,
                                        )
                                    except Exception as e:
                                        accelerator.print(f"[meta] best_FID info write skipped: {e}")
                                    if getattr(self, "tb", None) is not None:
                                        self.tb.add_scalar("best/FID", float(candidates["FID"]), global_step=self.step)

                                # ---------- ì •ì±… í‚¤ë“¤ì— ëŒ€í•´ ê°ê° best_{k} ì €ì¥ ----------
                                for k in keys:
                                    if k not in candidates:
                                        continue
                                    cur = float(candidates[k])
                                    if cur < self.best_scores.get(k, float("inf")):
                                        self.best_scores[k] = cur
                                        milestone = f"best_{k}"  # model-best_{k}.*
                                        self.save(milestone)
                                        did_save_best = True
                                        _improved[k] = cur
                                        try:
                                            self._record_ckpt_info(
                                                milestone,
                                                step=self.step,
                                                epoch=getattr(self, "epoch", None),
                                                metric_name=k,
                                                metric_value=cur,
                                                extra=extra_payload,
                                            )
                                        except Exception as e:
                                            accelerator.print(f"[meta] {milestone} info write skipped: {e}")
                                        if getattr(self, "tb", None) is not None:
                                            self.tb.add_scalar(f"best/{k}", cur, global_step=self.step)

                                # ---------- TB í…ìŠ¤íŠ¸ ìš”ì•½ ----------
                                if did_save_best and getattr(self, "tb", None) is not None:
                                    vt_s = f"{val_stats['val_total']:.6f}"        if val_stats and val_stats.get('val_total')        is not None else "N/A"
                                    vb_s = f"{val_stats['val_base']:.6f}"         if val_stats and val_stats.get('val_base')         is not None else "N/A"
                                    vm_s = f"{val_stats['val_mip']:.6f}"          if val_stats and val_stats.get('val_mip')          is not None else "N/A"
                                    vs_s = f"{val_stats['val_total_simple']:.6f}" if val_stats and val_stats.get('val_total_simple') is not None else "N/A"
                                    improved_txt = ", ".join(f"{k}={_improved[k]:.6f}" for k in sorted(_improved.keys()))
                                    self.tb.add_text(
                                        "best/info",
                                        f"step={self.step}, val_total={vt_s}, val_base={vb_s}, val_mip={vm_s}, val_total_simple={vs_s} | improved: {improved_txt}",
                                        global_step=self.step
                                    )
                                # (ADD) best/latest ì²˜ë¦¬ ëë‚œ ë’¤ì—ë„ ì£¼ê¸° ë²ˆí˜¸ ckp ì¶”ê°€ ì €ì¥
                                self.save(str(self.step // self.save_and_sample_every))
                            else:
                                self.save(milestone)




    def sample_on_val(
        self,
        out_dir: Optional[str] = None,
        max_items: Optional[int] = None,
        vis_save_mode: str = "center",   # "center" | "center+all" (í•„ìš”ì‹œ í™•ì¥)
        apply_stretch: bool = False,
        q_low: float = 0.01,
        q_high: float = 0.99,
    ):
        """
        ê²€ì¦ ë°ì´í„°ìŠ¤íŠ¸ë¦¼(self.val_dl)ì„ ìˆœíšŒí•˜ë©° ì…ë ¥/ì¡°ê±´/ìƒì„± ì´ë¯¸ì§€ë¥¼ ì €ì¥.
        - EMA ê°€ì¤‘ì¹˜ ì‚¬ìš© (self.ema.ema_model)
        - batch_size=1, shuffle=False ì „ì œ
        """
        import math, torch
        from torchvision import utils

        assert self.val_dl is not None, "val_dl ì´ ì—†ìŠµë‹ˆë‹¤. Trainer ìƒì„± ì‹œ eval_use_val_split=Trueì¸ì§€ í™•ì¸í•˜ì„¸ìš”."
        ema_model = self.ema.ema_model if hasattr(self, "ema") else self.model
        ema_model.eval()

        device = self.device
        results_dir = self.results_folder
        save_root = (results_dir / "val_samples") if out_dir is None else Path(out_dir)
        inputs_dir  = save_root / "inputs"
        samples_dir = save_root / "samples"
        cond_dir    = save_root / "conditions"
        for d in (inputs_dir, samples_dir, cond_dir):
            d.mkdir(parents=True, exist_ok=True)

        # ì‹œê°í™” í—¬í¼ (í•™ìŠµ ë£¨í”„ì™€ ë™ì¼ ì •ì±…)
        def _maybe_to_rgb(x):
            if x.dim() != 4: raise ValueError("expect [B,C,H,W]")
            return x.repeat(1,3,1,1) if x.size(1) == 1 else x

        def _percentile_stretch(x, lo, hi):
            # x: [B,C,H,W], per-image/channel ìŠ¤íŠ¸ë ˆì¹˜ (ê°„ë‹¨ë²„ì „)
            B,C,H,W = x.shape
            y = []
            for b in range(B):
                yc = []
                for c in range(C):
                    v = x[b,c]
                    lo_v = torch.quantile(v.flatten(), lo)
                    hi_v = torch.quantile(v.flatten(), hi)
                    vv = (v - lo_v) / max(hi_v - lo_v, 1e-6)
                    yc.append(vv.clamp(0,1)[None])
                y.append(torch.cat(yc, dim=0)[None])
            return torch.cat(y, dim=0)

        # ë©”íƒ€ë¥¼ ëŒë ¤ì£¼ëŠ”ì§€ ê°ì§€ (paired val ì€ ê¸°ë³¸ Falseë¼ index ê¸°ë°˜ ì´ë¦„ì„ ì”ë‹ˆë‹¤)
        return_meta = False
        try:
            first = next(iter(self.val_dl))
            # í˜•íƒœ ë³µì›
            if isinstance(first, (list, tuple)):
                maybe = first[0]
            else:
                maybe = first
            return_meta = isinstance(maybe, (tuple, list)) and len(maybe) == 2 and isinstance(maybe[1], dict)
        except Exception:
            pass

        # ë‹¤ì‹œ iterator ì›ë³µ
        val_iter = iter(self.val_dl)

        with torch.inference_mode():
            for idx in range(10**9):
                if max_items is not None and idx >= max_items:
                    break

                batch = next(val_iter, None)
                if batch is None:
                    break

                # -------- ë°°ì¹˜ íŒŒì‹± (tof, mgre, meta) --------
                if return_meta:
                    (tof, mgre), meta = batch
                else:
                    meta = None
                    tof, mgre = batch, None
                    if isinstance(batch, (tuple, list)) and len(batch) == 2:
                        tof, mgre = batch
                    elif isinstance(batch, dict) and 'tof' in batch and 'mgre' in batch:
                        tof, mgre = batch['tof'], batch['mgre']

                tof  = tof.to(device)
                # mgre = mgre.to(device) if mgre is not None else None
                mgre = (mgre.to(device) if (mgre is not None and torch.is_tensor(mgre)) else None)
                print(tof.shape, tof.dtype, tof.min().item(), tof.max().item())
                print(mgre.shape, mgre.dtype, mgre.min().item(), mgre.max().item()) if mgre is not None else None

                B, C, H, W = tof.shape
                assert B == 1, "val_dlì€ batch_size=1 ê°€ì •ì…ë‹ˆë‹¤."  # ìƒì„±ë¶€ê°€ ê°„ë‹¨í•´ì§
                center_idx = (C - 1) // 2

                # -------- ìƒì„± --------
                # condëŠ” ê·¸ëŒ€ë¡œ ì „ë‹¬(í•™ìŠµ ë£¨í”„ì™€ ë™ì¼)
                sample = ema_model.sample(batch_size=B, cond=mgre)  # [B,C,H,W], [0,1] ê°€ì •

                # -------- ì €ì¥ ì´ë¦„ êµ¬ì„± --------
                if meta is not None:
                    subj = meta.get("subject", "subj")
                    z    = meta.get("z", idx)
                    tag  = f"{subj}-z{int(z):04d}"
                else:
                    tag = f"idx{idx:05d}"

                # -------- ì„¼í„° ì±„ë„ ì €ì¥ (ì…ë ¥/ìƒ˜í”Œ/ì¡°ê±´) --------
                vis_c = tof[:, center_idx:center_idx+1]      # ì…ë ¥ì€ [-1,1]ì¼ ìˆ˜ ìˆìŒ
                samp_c = sample[:, center_idx:center_idx+1]  # ìƒ˜í”Œì€ [0,1] ê°€ì •

                # ========== âœ… NPY ì €ì¥(ì›ë³¸ í…ì„œ, ìŠ¤íŠ¸ë ˆì¹˜/ì»¬ëŸ¬ ë³€í™˜ ì´ì „) ==========
                if self.accelerator.is_main_process:
                    # í’€ ì±„ë„(ì›ë³¸)
                    np.save(inputs_dir  / f"val-input-full-{tag}.npy",  tof.detach().cpu().numpy().astype('float32'))
                    np.save(samples_dir / f"val-sample-full-{tag}.npy", sample.detach().cpu().numpy().astype('float32'))
                    # ì„¼í„° ì±„ë„(ì›ë³¸)
                    np.save(inputs_dir  / f"val-input-center-{tag}.npy",  vis_c.detach().cpu().numpy().astype('float32'))
                    np.save(samples_dir / f"val-sample-center-{tag}.npy", samp_c.detach().cpu().numpy().astype('float32'))
                # =====================================================================

                if apply_stretch:
                    vis_c  = _percentile_stretch(vis_c,  q_low, q_high)
                    samp_c = _percentile_stretch(samp_c, q_low, q_high)

                vis_c  = _maybe_to_rgb(vis_c)
                samp_c = _maybe_to_rgb(samp_c)

                # ì…ë ¥ì€ [-1,1] ìŠ¤ì¼€ì¼ ê°€ëŠ¥ì„± â†’ í•™ìŠµ ë£¨í”„ì™€ ë™ì¼í•˜ê²Œ normalize/value_rangeë¡œ ì €ì¥
                utils.save_image(
                    vis_c, str(inputs_dir / f"val-input-center-{tag}.png"),
                    nrow=1, normalize=True, value_range=(-1,1), scale_each=True
                )
                # ìƒ˜í”Œì€ [0,1] ê°€ì •ì´ì§€ë§Œ ë™ì¼ íŒŒì´í”„ë¼ì¸ ìœ ì§€(í•™ìŠµê³¼ ê°™ì€ ë£°)
                utils.save_image(
                    samp_c, str(samples_dir / f"val-sample-center-{tag}.png"),
                    nrow=1, normalize=True, value_range=(-1,1), scale_each=True
                )

                # condê°€ 5-ì±„ë„ ë¸”ë¡(ì²« ì—ì½”ì˜ 5 ì´ì›ƒ)ì´ë¼ë©´ ì¤‘ì•™ë§Œ ì €ì¥
                if mgre is not None and mgre.size(1) >= 5:
                    cond_5 = mgre[:, :5, ...]
                    cond_full = mgre[:, :, ...]
                    print('cond_5:', cond_5.shape, cond_5.dtype, cond_5.min().item(), cond_5.max().item())
                    print('cond_full:', cond_full.shape, cond_full.dtype, cond_full.min().item(), cond_full.max().item())
                    
                    cond_center_idx = (cond_5.shape[1] - 1) // 2
                    cond_c = cond_5[:, cond_center_idx:cond_center_idx+1]

                    # ===== âœ… NPY ì €ì¥(ì¡°ê±´, ì „ì²˜ë¦¬ ì´ì „) =====
                    if self.accelerator.is_main_process:
                        np.save(cond_dir / f"val-condition-slice-full-{tag}.npy",
                                cond_full.detach().cpu().numpy().astype('float32'))
                        np.save(cond_dir / f"val-condition-center-{tag}.npy",
                                cond_c.detach().cpu().numpy().astype('float32'))
                    # =========================================

                    if apply_stretch: # 
                        cond_c = _percentile_stretch(cond_c, q_low, q_high)
                    cond_c = _maybe_to_rgb(cond_c)
                    utils.save_image(
                        cond_c, str(cond_dir / f"val-condition-center-{tag}.png"),
                        nrow=1, normalize=True, value_range=(-1,1), scale_each=True
                    )

        # ë©€í‹°í”„ë¡œì„¸ìŠ¤ ì¤‘ë³µ ì €ì¥ ë°©ì§€: ì €ì¥ ìì²´ëŠ” main í”„ë¡œì„¸ìŠ¤ ê¶Œì¥
        # (ìœ„ ì½”ë“œë¥¼ ì „ì²´ë¥¼ if self.accelerator.is_main_process: ë¸”ë¡ìœ¼ë¡œ ê°ì‹¸ë„ ë©ë‹ˆë‹¤)



    #----------ìˆ˜ì • 2025-10-20: í…ŒìŠ¤íŠ¸ ì…‹ ìƒ˜í”Œë§ í•¨ìˆ˜ ì¶”ê°€(ê²€ì¦ê³¼ ë™ì¼ ì •ì±…)
    def sample_on_test(
        self,
        out_dir: Optional[str] = None,
        max_items: Optional[int] = None,
        vis_save_mode: str = "center",   # "center" | "center+all" (í•„ìš”ì‹œ í™•ì¥)
        apply_stretch: bool = False,
        q_low: float = 0.01,
        q_high: float = 0.99,
    ):
        """
        í…ŒìŠ¤íŠ¸ ë°ì´í„°ìŠ¤íŠ¸ë¦¼(self.test_dl)ì„ ìˆœíšŒí•˜ë©° ì…ë ¥/ì¡°ê±´/ìƒì„± ì´ë¯¸ì§€ë¥¼ ì €ì¥.
        - EMA ê°€ì¤‘ì¹˜ ì‚¬ìš© (self.ema.ema_model)
        - batch_size=1, shuffle=False ì „ì œ
        - sample_on_valê³¼ ë™ì¼ ì •ì±…/êµ¬ì¡°, ì €ì¥ ë£¨íŠ¸ì™€ íŒŒì¼ ì ‘ë‘ì–´ë§Œ 'test'ë¡œ ë³€ê²½
        """
        import math, torch
        from torchvision import utils

        assert getattr(self, "test_dl", None) is not None, "test_dl ì´ ì—†ìŠµë‹ˆë‹¤. Trainer ìƒì„± ì‹œ eval_use_test_split=Trueì¸ì§€ í™•ì¸í•˜ì„¸ìš”."
        ema_model = self.ema.ema_model if hasattr(self, "ema") else self.model
        ema_model.eval()

        device = self.device
        results_dir = self.results_folder
        save_root = (results_dir / "test_samples") if out_dir is None else Path(out_dir)   # â† test ì „ìš© í´ë”
        inputs_dir  = save_root / "inputs"
        samples_dir = save_root / "samples"
        cond_dir    = save_root / "conditions"
        for d in (inputs_dir, samples_dir, cond_dir):
            d.mkdir(parents=True, exist_ok=True)

        # ì‹œê°í™” í—¬í¼ (valê³¼ ë™ì¼)
        def _maybe_to_rgb(x):
            if x.dim() != 4: raise ValueError("expect [B,C,H,W]")
            return x.repeat(1,3,1,1) if x.size(1) == 1 else x

        def _percentile_stretch(x, lo, hi):
            # x: [B,C,H,W], per-image/channel ìŠ¤íŠ¸ë ˆì¹˜ (ê°„ë‹¨ë²„ì „)
            B,C,H,W = x.shape
            y = []
            for b in range(B):
                yc = []
                for c in range(C):
                    v = x[b,c]
                    lo_v = torch.quantile(v.flatten(), lo)
                    hi_v = torch.quantile(v.flatten(), hi)
                    vv = (v - lo_v) / max(hi_v - lo_v, 1e-6)
                    yc.append(vv.clamp(0,1)[None])
                y.append(torch.cat(yc, dim=0)[None])
            return torch.cat(y, dim=0)

        # ë©”íƒ€ ë°˜í™˜ ê°ì§€
        return_meta = False
        try:
            first = next(iter(self.test_dl))
            maybe = first[0] if isinstance(first, (list, tuple)) else first
            return_meta = isinstance(maybe, (tuple, list)) and len(maybe) == 2 and isinstance(maybe[1], dict)
        except Exception:
            pass

        test_iter = iter(self.test_dl)

        with torch.inference_mode():
            for idx in range(10**9):
                if max_items is not None and idx >= max_items:
                    break

                batch = next(test_iter, None)
                if batch is None:
                    break

                # -------- ë°°ì¹˜ íŒŒì‹± (tof, mgre, meta) --------
                if return_meta:
                    (tof, mgre), meta = batch
                else:
                    meta = None
                    tof, mgre = batch, None
                    if isinstance(batch, (tuple, list)) and len(batch) == 2:
                        tof, mgre = batch
                    elif isinstance(batch, dict) and 'tof' in batch and 'mgre' in batch:
                        tof, mgre = batch['tof'], batch['mgre']

                tof  = tof.to(device)
                # mgre = mgre.to(device) if mgre is not None else None
                mgre = (mgre.to(device) if (mgre is not None and torch.is_tensor(mgre)) else None)
                print(tof.shape, tof.dtype, tof.min().item(), tof.max().item())
                print(mgre.shape, mgre.dtype, mgre.min().item(), mgre.max().item()) if mgre is not None else None

                B, C, H, W = tof.shape
                assert B == 1, "test_dlì€ batch_size=1 ê°€ì •ì…ë‹ˆë‹¤."
                center_idx = (C - 1) // 2

                # -------- ìƒì„± --------
                #-------------------ëª¨ë“ ìƒ˜í”Œì €ì¥í•˜ê¸° (ì‹œì‘)
                sample_all = ema_model.sample(batch_size=B, cond=mgre, return_all_timesteps=True)  # [B,T,C,H,W]
                sample = sample_all[:, -1, ...]  # [B,C,H,W]
                #-------------------ëª¨ë“ ìƒ˜í”Œì €ì¥í•˜ê¸° (ë)

                # -------- ì €ì¥ ì´ë¦„ êµ¬ì„± --------
                if meta is not None:
                    subj = meta.get("subject", "subj")
                    z    = meta.get("z", idx)
                    tag  = f"{subj}-z{int(z):04d}"
                else:
                    tag = f"idx{idx:05d}"

                #-------------------í¬ì›Œë“œë…¸ì´ì¦ˆì‹œê°í™” (ì‹œì‘)
                # ì…ë ¥/ì¡°ê±´ì˜ ì •ë°©í–¥(í¬ì›Œë“œ) ë…¸ì´ì¦ˆ ê³¼ì •ì„ ê°™ì€ ìŠ¤ì¼€ì¤„ë¡œ ì‹œê°í™” (t001..tT)
                try:
                    # alphas_cumprod ì¤€ë¹„
                    if hasattr(ema_model, "alphas_cumprod") and ema_model.alphas_cumprod is not None:
                        acp = ema_model.alphas_cumprod.to(device)                 # [num_train_steps]
                    elif hasattr(ema_model, "betas") and ema_model.betas is not None:
                        betas = ema_model.betas.to(device)
                        acp = torch.cumprod(1. - betas, dim=0)                    # [num_train_steps]
                    else:
                        acp = None

                    T = sample_all.shape[1] - 1                                    # ì´ˆê¸°ë…¸ì´ì¦ˆ ì œì™¸í•œ ê¸¸ì´(ì˜ˆ: 250)

                    # DDIM ì„œë¸ŒìŠ¤í… ë§¤í•‘ ê°€ì ¸ì˜¤ê¸° (ê°€ëŠ¥í•˜ë©´)
                    if hasattr(ema_model, "ddim_timesteps") and ema_model.ddim_timesteps is not None:
                        t_indices = ema_model.ddim_timesteps.to(device).long()
                        if t_indices.numel() != T:
                            num_train = int(getattr(ema_model, "num_timesteps", acp.numel() if acp is not None else T))
                            t_indices = torch.linspace(1, num_train - 1, steps=T, device=device).long()
                    else:
                        num_train = int(getattr(ema_model, "num_timesteps", acp.numel() if acp is not None else T))
                        t_indices = torch.linspace(1, num_train - 1, steps=T, device=device).long()

                    if acp is not None and self.accelerator.is_main_process:
                        #-------------------í¬ì›Œë“œë…¸ì´ì¦ˆì‹œê°í™” (ìˆ˜ì •: torch ë²„ì „ í˜¸í™˜ ë‚œìˆ˜)
                        def _seed_global(seed: int, is_cuda: bool):
                            if is_cuda:
                                torch.cuda.manual_seed_all(seed)
                            else:
                                torch.manual_seed(seed)

                        # 1) ì…ë ¥(ì„¼í„° ì±„ë„) í¬ì›Œë“œ
                        fwd_in_dir = (inputs_dir / f"{tag}_forward")
                        fwd_in_dir.mkdir(parents=True, exist_ok=True)

                        x0 = tof[:, center_idx:center_idx+1].detach()             # [B,1,H,W]  ([-1,1] ê°€ì •)
                        seed_x = int(str(abs(hash(tag)))[:8], 10)
                        _seed_global(seed_x, x0.is_cuda)
                        eps = torch.randn_like(x0)                                 # <-- generator ì¸ì ì—†ì´ ì „ì—­ ì‹œë“œ ì‚¬ìš©

                        for k, t in enumerate(t_indices.tolist(), start=1):
                            at = acp[t].sqrt().view(1,1,1,1)
                            nt = (1. - acp[t]).sqrt().view(1,1,1,1)
                            xt = at * x0 + nt * eps                                # [B,1,H,W]
                            xvis = _percentile_stretch(xt, q_low, q_high) if apply_stretch else xt
                            xvis = _maybe_to_rgb(xvis)
                            utils.save_image(
                                xvis, str(fwd_in_dir / f"t{k:03d}.png"),
                                nrow=1, normalize=True, value_range=(-1,1), scale_each=True
                            )

                        # 2) ì»¨ë””ì…˜(ì„¼í„° ì±„ë„) í¬ì›Œë“œ (ìˆì„ ë•Œ)
                        if (mgre is not None) and (mgre.size(1) >= 1):
                            fwd_cond_dir = (cond_dir / f"{tag}_forward")
                            fwd_cond_dir.mkdir(parents=True, exist_ok=True)

                            if mgre.size(1) >= 5:
                                cond_center_idx = (min(5, mgre.size(1)) - 1) // 2
                            else:
                                cond_center_idx = (mgre.size(1) - 1) // 2
                            c0 = mgre[:, cond_center_idx:cond_center_idx+1].detach()  # [B,1,H,W]

                            seed_c = int(str(abs(hash(tag + "_cond")))[:8], 10)
                            _seed_global(seed_c, c0.is_cuda)
                            eps2 = torch.randn_like(c0)                               # <-- ë™ì¼í•˜ê²Œ ì „ì—­ ì‹œë“œ ì‚¬ìš©

                            for k, t in enumerate(t_indices.tolist(), start=1):
                                at = acp[t].sqrt().view(1,1,1,1)
                                nt = (1. - acp[t]).sqrt().view(1,1,1,1)
                                ct = at * c0 + nt * eps2                              # [B,1,H,W]
                                cvis = _percentile_stretch(ct, q_low, q_high) if apply_stretch else ct
                                cvis = _maybe_to_rgb(cvis)
                                utils.save_image(
                                    cvis, str(fwd_cond_dir / f"t{k:03d}.png"),
                                    nrow=1, normalize=True, value_range=(-1,1), scale_each=True
                                )
                except Exception as e:
                    # í¬ì›Œë“œ ì‹œê°í™”ëŠ” ë³´ì¡° ê¸°ëŠ¥ì´ë¯€ë¡œ ì‹¤íŒ¨í•´ë„ ì§„í–‰
                    print(f"[Forward-Vis skipped] {e}")
                #-------------------í¬ì›Œë“œë…¸ì´ì¦ˆì‹œê°í™” (ë)

                #-------------------ëª¨ë“ ìƒ˜í”Œì €ì¥í•˜ê¸° (ê³„ì†)
                if self.accelerator.is_main_process:
                    step_out_dir = (samples_dir / f"{tag}_steps")
                    step_out_dir.mkdir(parents=True, exist_ok=True)
                    for t in range(1, sample_all.shape[1]):  # ì´ˆê¸°ë…¸ì´ì¦ˆ(0) ì œì™¸ â†’ ì •í™•íˆ 250ì¥
                        # !! ë°°ì¹˜ ì°¨ì› ìœ ì§€ !!
                        step_c = sample_all[:, t, center_idx:center_idx+1, ...]  # [B,1,H,W]
                        if apply_stretch:
                            step_c = _percentile_stretch(step_c, q_low, q_high)
                        step_c = _maybe_to_rgb(step_c)  # [B,3,H,W]
                        utils.save_image(
                            step_c, str(step_out_dir / f"t{t:03d}.png"),
                            nrow=1, normalize=True, value_range=(-1,1)  # ìµœì¢… ì €ì¥ê³¼ ë™ì¼ ìŠ¤ì¼€ì¼
                        )
                #-------------------ëª¨ë“ ìƒ˜í”Œì €ì¥í•˜ê¸° (ë)

                # -------- ì„¼í„° ì±„ë„ ì €ì¥ (ì…ë ¥/ìƒ˜í”Œ/ì¡°ê±´) --------
                vis_c  = tof[:, center_idx:center_idx+1]
                samp_c = sample[:, center_idx:center_idx+1]

                # ========== âœ… NPY ì €ì¥(ì›ë³¸ í…ì„œ, ìŠ¤íŠ¸ë ˆì¹˜/ì»¬ëŸ¬ ë³€í™˜ ì´ì „) ==========
                if self.accelerator.is_main_process:
                    np.save(inputs_dir  / f"test-input-full-{tag}.npy",   tof.detach().cpu().numpy().astype('float32'))      # â† 'test-' ì ‘ë‘ì–´
                    np.save(samples_dir / f"test-sample-full-{tag}.npy",  sample.detach().cpu().numpy().astype('float32'))   # â† 'test-' ì ‘ë‘ì–´
                    np.save(inputs_dir  / f"test-input-center-{tag}.npy",  vis_c.detach().cpu().numpy().astype('float32'))   # â† 'test-' ì ‘ë‘ì–´
                    np.save(samples_dir / f"test-sample-center-{tag}.npy", samp_c.detach().cpu().numpy().astype('float32'))  # â† 'test-' ì ‘ë‘ì–´
                # ====================================================================

                if apply_stretch: 
                    vis_c  = _percentile_stretch(vis_c,  q_low, q_high)
                    samp_c = _percentile_stretch(samp_c, q_low, q_high)

                vis_c  = _maybe_to_rgb(vis_c)
                samp_c = _maybe_to_rgb(samp_c)

                utils.save_image(
                    vis_c, str(inputs_dir / f"test-input-center-{tag}.png"),
                    nrow=1, normalize=True, value_range=(-1,1), scale_each=True
                )
                utils.save_image(
                    samp_c, str(samples_dir / f"test-sample-center-{tag}.png"),
                    nrow=1, normalize=True, value_range=(-1,1), scale_each=True
                )

                # cond ì €ì¥(ìˆë‹¤ë©´)
                if mgre is not None and mgre.size(1) >= 5:
                    cond_5 = mgre[:, :5, ...]
                    cond_full = mgre[:, :, ...]
                    print('cond_5:', cond_5.shape, cond_5.dtype, cond_5.min().item(), cond_5.max().item())
                    print('cond_full:', cond_full.shape, cond_full.dtype, cond_full.min().item(), cond_full.max().item())
                    
                    cond_center_idx = (cond_5.shape[1] - 1) // 2
                    cond_c = cond_5[:, cond_center_idx:cond_center_idx+1]

                    # ===== âœ… NPY ì €ì¥(ì¡°ê±´, ì „ì²˜ë¦¬ ì´ì „) =====
                    if self.accelerator.is_main_process:
                        np.save(cond_dir / f"test-condition-slice-full-{tag}.npy",
                                cond_full.detach().cpu().numpy().astype('float32'))            # â† 'test-' ì ‘ë‘ì–´
                        np.save(cond_dir / f"test-condition-center-{tag}.npy",
                                cond_c.detach().cpu().numpy().astype('float32'))               # â† 'test-' ì ‘ë‘ì–´
                    # =========================================

                    if apply_stretch:
                        cond_c = _percentile_stretch(cond_c, q_low, q_high)
                    cond_c = _maybe_to_rgb(cond_c)
                    utils.save_image(
                        cond_c, str(cond_dir / f"test-condition-center-{tag}.png"),
                        nrow=1, normalize=True, value_range=(-1,1), scale_each=True
                    )
    #----------ìˆ˜ì • 2025-10-20 ë




    # #----------ìˆ˜ì • 2025-10-20: í…ŒìŠ¤íŠ¸ ì…‹ ìƒ˜í”Œë§ í•¨ìˆ˜ ì¶”ê°€(ê²€ì¦ê³¼ ë™ì¼ ì •ì±…)
    # def sample_on_test(
    #     self,
    #     out_dir: Optional[str] = None,
    #     max_items: Optional[int] = None,
    #     vis_save_mode: str = "center",   # "center" | "center+all" (í•„ìš”ì‹œ í™•ì¥)
    #     apply_stretch: bool = False,
    #     q_low: float = 0.01,
    #     q_high: float = 0.99,
    # ):
    #     """
    #     í…ŒìŠ¤íŠ¸ ë°ì´í„°ìŠ¤íŠ¸ë¦¼(self.test_dl)ì„ ìˆœíšŒí•˜ë©° ì…ë ¥/ì¡°ê±´/ìƒì„± ì´ë¯¸ì§€ë¥¼ ì €ì¥.
    #     - EMA ê°€ì¤‘ì¹˜ ì‚¬ìš© (self.ema.ema_model)
    #     - batch_size=1, shuffle=False ì „ì œ
    #     - sample_on_valê³¼ ë™ì¼ ì •ì±…/êµ¬ì¡°, ì €ì¥ ë£¨íŠ¸ì™€ íŒŒì¼ ì ‘ë‘ì–´ë§Œ 'test'ë¡œ ë³€ê²½
    #     """
    #     import math, torch
    #     from torchvision import utils

    #     assert getattr(self, "test_dl", None) is not None, "test_dl ì´ ì—†ìŠµë‹ˆë‹¤. Trainer ìƒì„± ì‹œ eval_use_test_split=Trueì¸ì§€ í™•ì¸í•˜ì„¸ìš”."
    #     ema_model = self.ema.ema_model if hasattr(self, "ema") else self.model
    #     ema_model.eval()

    #     device = self.device
    #     results_dir = self.results_folder
    #     save_root = (results_dir / "test_samples") if out_dir is None else Path(out_dir)   # â† test ì „ìš© í´ë”
    #     inputs_dir  = save_root / "inputs"
    #     samples_dir = save_root / "samples"
    #     cond_dir    = save_root / "conditions"
    #     for d in (inputs_dir, samples_dir, cond_dir):
    #         d.mkdir(parents=True, exist_ok=True)

    #     # ì‹œê°í™” í—¬í¼ (valê³¼ ë™ì¼)
    #     def _maybe_to_rgb(x):
    #         if x.dim() != 4: raise ValueError("expect [B,C,H,W]")
    #         return x.repeat(1,3,1,1) if x.size(1) == 1 else x

    #     def _percentile_stretch(x, lo, hi):
    #         # x: [B,C,H,W], per-image/channel ìŠ¤íŠ¸ë ˆì¹˜ (ê°„ë‹¨ë²„ì „)
    #         B,C,H,W = x.shape
    #         y = []
    #         for b in range(B):
    #             yc = []
    #             for c in range(C):
    #                 v = x[b,c]
    #                 lo_v = torch.quantile(v.flatten(), lo)
    #                 hi_v = torch.quantile(v.flatten(), hi)
    #                 vv = (v - lo_v) / max(hi_v - lo_v, 1e-6)
    #                 yc.append(vv.clamp(0,1)[None])
    #             y.append(torch.cat(yc, dim=0)[None])
    #         return torch.cat(y, dim=0)

    #     # ë©”íƒ€ ë°˜í™˜ ê°ì§€
    #     return_meta = False
    #     try:
    #         first = next(iter(self.test_dl))
    #         maybe = first[0] if isinstance(first, (list, tuple)) else first
    #         return_meta = isinstance(maybe, (tuple, list)) and len(maybe) == 2 and isinstance(maybe[1], dict)
    #     except Exception:
    #         pass

    #     test_iter = iter(self.test_dl)

    #     with torch.inference_mode():
    #         for idx in range(10**9):
    #             if max_items is not None and idx >= max_items:
    #                 break

    #             batch = next(test_iter, None)
    #             if batch is None:
    #                 break

    #             # -------- ë°°ì¹˜ íŒŒì‹± (tof, mgre, meta) --------
    #             if return_meta:
    #                 (tof, mgre), meta = batch
    #             else:
    #                 meta = None
    #                 tof, mgre = batch, None
    #                 if isinstance(batch, (tuple, list)) and len(batch) == 2:
    #                     tof, mgre = batch
    #                 elif isinstance(batch, dict) and 'tof' in batch and 'mgre' in batch:
    #                     tof, mgre = batch['tof'], batch['mgre']

    #             tof  = tof.to(device)
    #             # mgre = mgre.to(device) if mgre is not None else None
    #             mgre = (mgre.to(device) if (mgre is not None and torch.is_tensor(mgre)) else None)
    #             print(tof.shape, tof.dtype, tof.min().item(), tof.max().item())
    #             print(mgre.shape, mgre.dtype, mgre.min().item(), mgre.max().item()) if mgre is not None else None

    #             B, C, H, W = tof.shape
    #             assert B == 1, "test_dlì€ batch_size=1 ê°€ì •ì…ë‹ˆë‹¤."
    #             center_idx = (C - 1) // 2

    #             # -------- ìƒì„± --------
    #             sample = ema_model.sample(batch_size=B, cond=mgre)  # [B,C,H,W], [0,1] ê°€ì •

    #             # -------- ì €ì¥ ì´ë¦„ êµ¬ì„± --------
    #             if meta is not None:
    #                 subj = meta.get("subject", "subj")
    #                 z    = meta.get("z", idx)
    #                 tag  = f"{subj}-z{int(z):04d}"
    #             else:
    #                 tag = f"idx{idx:05d}"

    #             # -------- ì„¼í„° ì±„ë„ ì €ì¥ (ì…ë ¥/ìƒ˜í”Œ/ì¡°ê±´) --------
    #             vis_c  = tof[:, center_idx:center_idx+1]
    #             samp_c = sample[:, center_idx:center_idx+1]

    #             # ========== âœ… NPY ì €ì¥(ì›ë³¸ í…ì„œ, ìŠ¤íŠ¸ë ˆì¹˜/ì»¬ëŸ¬ ë³€í™˜ ì´ì „) ==========
    #             if self.accelerator.is_main_process:
    #                 # í’€ ì±„ë„(ì›ë³¸)
    #                 np.save(inputs_dir  / f"test-input-full-{tag}.npy",   tof.detach().cpu().numpy().astype('float32'))      # â† 'test-' ì ‘ë‘ì–´
    #                 np.save(samples_dir / f"test-sample-full-{tag}.npy",  sample.detach().cpu().numpy().astype('float32'))   # â† 'test-' ì ‘ë‘ì–´
    #                 # ì„¼í„° ì±„ë„(ì›ë³¸)
    #                 np.save(inputs_dir  / f"test-input-center-{tag}.npy",  vis_c.detach().cpu().numpy().astype('float32'))   # â† 'test-' ì ‘ë‘ì–´
    #                 np.save(samples_dir / f"test-sample-center-{tag}.npy", samp_c.detach().cpu().numpy().astype('float32'))  # â† 'test-' ì ‘ë‘ì–´
    #             # ====================================================================

    #             if apply_stretch:
    #                 vis_c  = _percentile_stretch(vis_c,  q_low, q_high)
    #                 samp_c = _percentile_stretch(samp_c, q_low, q_high)

    #             vis_c  = _maybe_to_rgb(vis_c)
    #             samp_c = _maybe_to_rgb(samp_c)

    #             # ì…ë ¥ì€ [-1,1] ìŠ¤ì¼€ì¼ ê°€ëŠ¥ì„± â†’ valê³¼ ë™ì¼ íŒŒì´í”„ë¼ì¸ ìœ ì§€
    #             utils.save_image(
    #                 vis_c, str(inputs_dir / f"test-input-center-{tag}.png"),    # â† 'test-' ì ‘ë‘ì–´
    #                 nrow=1, normalize=True, value_range=(-1,1), scale_each=True
    #             )
    #             utils.save_image(
    #                 samp_c, str(samples_dir / f"test-sample-center-{tag}.png"), # â† 'test-' ì ‘ë‘ì–´
    #                 nrow=1, normalize=True, value_range=(-1,1), scale_each=True
    #             )

    #             # cond ì €ì¥(ìˆë‹¤ë©´)
    #             if mgre is not None and mgre.size(1) >= 5:
    #                 cond_5 = mgre[:, :5, ...]
    #                 cond_full = mgre[:, :, ...]
    #                 print('cond_5:', cond_5.shape, cond_5.dtype, cond_5.min().item(), cond_5.max().item())
    #                 print('cond_full:', cond_full.shape, cond_full.dtype, cond_full.min().item(), cond_full.max().item())
                    
    #                 cond_center_idx = (cond_5.shape[1] - 1) // 2
    #                 cond_c = cond_5[:, cond_center_idx:cond_center_idx+1]

    #                 # ===== âœ… NPY ì €ì¥(ì¡°ê±´, ì „ì²˜ë¦¬ ì´ì „) =====
    #                 if self.accelerator.is_main_process:
    #                     np.save(cond_dir / f"test-condition-slice-full-{tag}.npy",
    #                             cond_full.detach().cpu().numpy().astype('float32'))            # â† 'test-' ì ‘ë‘ì–´
    #                     np.save(cond_dir / f"test-condition-center-{tag}.npy",
    #                             cond_c.detach().cpu().numpy().astype('float32'))               # â† 'test-' ì ‘ë‘ì–´
    #                 # =========================================

    #                 if apply_stretch:
    #                     cond_c = _percentile_stretch(cond_c, q_low, q_high)
    #                 cond_c = _maybe_to_rgb(cond_c)
    #                 utils.save_image(
    #                     cond_c, str(cond_dir / f"test-condition-center-{tag}.png"),           # â† 'test-' ì ‘ë‘ì–´
    #                     nrow=1, normalize=True, value_range=(-1,1), scale_each=True
    #                 )
    # #----------ìˆ˜ì • 2025-10-20 ë
